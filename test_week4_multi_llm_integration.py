#!/usr/bin/env python3
"""
Test Week 4: Multi-LLM Provider Integration

This script comprehensively tests the Week 4 implementation of multi-LLM
provider integration with physics-informed context routing and response fusion.

Test Coverage:
- LLM Provider Manager functionality
- Multi-LLM Agent coordination
- Physics-informed context routing
- Response fusion and consensus building
- Performance optimization across providers
- Cost efficiency and load balancing
"""

import sys
import os
import asyncio
import numpy as np
import time

# Add the project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_llm_provider_manager():
    """Test the LLM Provider Manager implementation."""
    print("ü§ñ Testing LLM Provider Manager...")
    
    try:
        from src.llm.providers.llm_provider_manager import (
            LLMProviderManager, PhysicsInformedContext, TaskType, LLMProvider
        )
        
        manager = LLMProviderManager()
        
        # Test 1: Provider initialization
        stats = manager.get_global_statistics()
        print(f"   ‚úÖ Initialized with {stats['active_providers']}/{stats['total_providers']} providers")
        
        # Test 2: Single provider response
        context = PhysicsInformedContext(
            original_prompt="Analyze the energy conservation in this oscillating system",
            physics_compliance=0.72,  # Realistic test value, not "impressive"
            symbolic_functions=["sin(2*pi*t)*exp(-0.1*t)"],
            scientific_insights=["Damped harmonic oscillator", "Energy dissipation present"],
            integrity_score=0.68,  # Realistic test value
            task_type=TaskType.SCIENTIFIC_ANALYSIS
        )
        
        async def test_single_response():
            response = await manager.generate_response(context, use_fusion=False)
            return response
        
        single_response = asyncio.run(test_single_response())
        
        if single_response.error is None:
            print(f"   ‚úÖ Single provider response successful")
            print(f"   üìä Provider: {single_response.provider.value}")
            print(f"   üìä Confidence: {single_response.confidence:.3f}")
            print(f"   üí∞ Cost: ${single_response.cost:.4f}")
            print(f"   ‚è±Ô∏è Time: {single_response.processing_time:.3f}s")
        else:
            print(f"   ‚ùå Single provider response failed: {single_response.error}")
            return False
        
        # Test 3: Multi-provider fusion
        async def test_fusion_response():
            fused_response = await manager.generate_response(context, use_fusion=True, max_providers=3)
            return fused_response
        
        fused_response = asyncio.run(test_fusion_response())
        
        print(f"   ‚úÖ Multi-provider fusion successful")
        print(f"   ü§ñ Providers: {len(fused_response.contributing_providers)}")
        print(f"   üìä Consensus: {fused_response.consensus_score:.3f}")
        print(f"   üìä Confidence: {fused_response.confidence:.3f}")
        print(f"   üí∞ Total cost: ${fused_response.total_cost:.4f}")
        print(f"   ‚öñÔ∏è Physics validated: {fused_response.physics_validated}")
        
        # Test 4: Provider performance
        for provider_type in [LLMProvider.GPT4_1, LLMProvider.CLAUDE4, LLMProvider.GEMINI_PRO]:
            perf = manager.get_provider_performance(provider_type)
            if perf:
                print(f"   üìà {provider_type.value}: {perf['performance']['total_requests']} requests")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå LLM Provider Manager test failed: {e}")
        return False

def test_multi_llm_agent():
    """Test the Multi-LLM Agent coordination."""
    print("\nüé≠ Testing Multi-LLM Agent...")
    
    try:
        from src.agents.coordination.multi_llm_agent import MultiLLMAgent, MultiLLMStrategy, ValidationLevel
        
        agent = MultiLLMAgent()
        
        # Test 1: Physics-informed coordination
        test_message = {
            "operation": "coordinate",
            "payload": {
                "prompt": "Evaluate the thermodynamic efficiency of this heat engine cycle",
                "task_type": "scientific_analysis",
                "strategy": "physics_informed",
                "validation_level": "standard",
                "max_providers": 3,
                "scientific_result": {
                    "physics_compliance": 0.78,  # Below threshold
                    "physics_violations": ["potential entropy decrease", "energy conservation anomaly"],
                    "symbolic_functions": ["Q_hot/(Q_hot - Q_cold)", "1 - T_cold/T_hot"],
                    "scientific_insights": ["Carnot cycle analysis", "Thermodynamic constraints"],
                    "integrity_score": 0.82
                },
                "physics_requirements": {"min_compliance": 0.8}
            }
        }
        
        result = agent.process(test_message)
        
        if result["status"] == "success":
            payload = result["payload"]
            print(f"   ‚úÖ Physics-informed coordination successful")
            print(f"   üìä Confidence: {payload['confidence']:.3f}")
            print(f"   ü§ù Consensus: {payload['consensus_score']:.3f}")
            print(f"   üß™ Physics compliance: {payload['physics_compliance']:.3f}")
            print(f"   ü§ñ Providers used: {len(payload['providers_used'])}")
            print(f"   üìã Strategy: {payload['strategy_used']}")
            print(f"   ‚è±Ô∏è Processing time: {payload['processing_time']:.3f}s")
            print(f"   üí° Recommendations: {len(payload['recommendations'])}")
        else:
            print(f"   ‚ùå Physics-informed coordination failed: {result['payload']}")
            return False
        
        # Test 2: Consensus strategy
        consensus_message = {
            "operation": "coordinate",
            "payload": {
                "prompt": "Determine the optimal solution approach for this differential equation",
                "task_type": "mathematical_reasoning",
                "strategy": "consensus",
                "validation_level": "rigorous",
                "max_providers": 4,
                "scientific_result": {
                    "physics_compliance": 0.92,
                    "physics_violations": [],
                    "symbolic_functions": ["d¬≤y/dx¬≤ + œâ¬≤y = 0"],
                    "scientific_insights": ["Second-order linear ODE", "Harmonic oscillator equation"],
                    "integrity_score": 0.95
                }
            }
        }
        
        consensus_result = agent.process(consensus_message)
        
        if consensus_result["status"] == "success":
            payload = consensus_result["payload"]
            print(f"   ‚úÖ Consensus strategy successful")
            print(f"   ü§ù Consensus score: {payload['consensus_score']:.3f}")
            print(f"   üìä Final confidence: {payload['confidence']:.3f}")
        else:
            print(f"   ‚ùå Consensus strategy failed")
            return False
        
        # Test 3: Creative fusion strategy
        creative_message = {
            "operation": "coordinate",
            "payload": {
                "prompt": "Explore novel applications of quantum tunneling in energy systems",
                "task_type": "creative_exploration",
                "strategy": "creative_fusion",
                "validation_level": "standard",
                "max_providers": 3,
                "scientific_result": {
                    "physics_compliance": 0.88,
                    "physics_violations": [],
                    "symbolic_functions": ["œà(x) = A*exp(-kx)", "T = exp(-2‚àö(2m(V-E))a/‚Ñè)"],
                    "scientific_insights": ["Quantum mechanics", "Tunneling probability"],
                    "integrity_score": 0.91
                }
            }
        }
        
        creative_result = agent.process(creative_message)
        
        if creative_result["status"] == "success":
            payload = creative_result["payload"]
            print(f"   ‚úÖ Creative fusion strategy successful")
            print(f"   üé® Creative consensus: {payload['consensus_score']:.3f}")
            print(f"   üìä Fused confidence: {payload['confidence']:.3f}")
        else:
            print(f"   ‚ùå Creative fusion strategy failed")
            return False
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Multi-LLM Agent test failed: {e}")
        return False

def test_provider_task_specialization():
    """Test provider specialization for different task types."""
    print("\nüéØ Testing Provider Task Specialization...")
    
    try:
        from src.llm.providers.llm_provider_manager import (
            LLMProviderManager, PhysicsInformedContext, TaskType
        )
        
        manager = LLMProviderManager()
        
        # Test different task types
        task_scenarios = [
            {
                "name": "Scientific Analysis",
                "task_type": TaskType.SCIENTIFIC_ANALYSIS,
                "prompt": "Analyze the stability of this nonlinear dynamical system",
                "physics_compliance": 0.89
            },
            {
                "name": "Physics Validation",
                "task_type": TaskType.PHYSICS_VALIDATION,
                "prompt": "Validate the energy conservation in this proposed mechanism",
                "physics_compliance": 0.65  # Low compliance needs validation
            },
            {
                "name": "Creative Exploration",
                "task_type": TaskType.CREATIVE_EXPLORATION,
                "prompt": "Explore unconventional approaches to energy harvesting",
                "physics_compliance": 0.95
            },
            {
                "name": "System Coordination",
                "task_type": TaskType.SYSTEM_COORDINATION,
                "prompt": "Coordinate multi-agent system for distributed optimization",
                "physics_compliance": 0.82
            }
        ]
        
        results = []
        
        for scenario in task_scenarios:
            context = PhysicsInformedContext(
                original_prompt=scenario["prompt"],
                physics_compliance=scenario["physics_compliance"],
                symbolic_functions=["f(x) = sin(x)"],
                scientific_insights=["Test scenario"],
                integrity_score=0.85,
                task_type=scenario["task_type"]
            )
            
            async def test_scenario():
                response = await manager.generate_response(context, use_fusion=False)
                return response
            
            response = asyncio.run(test_scenario())
            
            results.append({
                "task": scenario["name"],
                "provider": response.provider.value,
                "confidence": response.confidence,
                "processing_time": response.processing_time,
                "cost": response.cost,
                "success": response.error is None
            })
            
            print(f"   üìã {scenario['name']}: {response.provider.value} (conf: {response.confidence:.3f})")
        
        # Analyze specialization
        success_rate = sum(1 for r in results if r["success"]) / len(results)
        avg_confidence = np.mean([r["confidence"] for r in results if r["success"]])
        avg_time = np.mean([r["processing_time"] for r in results if r["success"]])
        total_cost = sum(r["cost"] for r in results if r["success"])
        
        print(f"   üìä Specialization Results:")
        print(f"     ‚Ä¢ Success rate: {success_rate:.1%}")
        print(f"     ‚Ä¢ Average confidence: {avg_confidence:.3f}")
        print(f"     ‚Ä¢ Average time: {avg_time:.3f}s")
        print(f"     ‚Ä¢ Total cost: ${total_cost:.4f}")
        
        return success_rate > 0.8
        
    except Exception as e:
        print(f"   ‚ùå Provider specialization test failed: {e}")
        return False

def test_physics_informed_routing():
    """Test physics-informed context routing."""
    print("\n‚öñÔ∏è Testing Physics-Informed Routing...")
    
    try:
        from src.agents.coordination.multi_llm_agent import MultiLLMAgent
        
        agent = MultiLLMAgent()
        
        # Test scenarios with different physics compliance levels
        physics_scenarios = [
            {
                "name": "High Physics Compliance",
                "physics_compliance": 0.95,
                "violations": [],
                "expected_routing": "standard processing"
            },
            {
                "name": "Low Physics Compliance",
                "physics_compliance": 0.65,
                "violations": ["energy conservation violation", "causality issue"],
                "expected_routing": "validation-focused processing"
            },
            {
                "name": "Medium Physics Compliance",
                "physics_compliance": 0.80,
                "violations": ["minor continuity issue"],
                "expected_routing": "balanced processing"
            }
        ]
        
        routing_results = []
        
        for scenario in physics_scenarios:
            test_message = {
                "operation": "coordinate",
                "payload": {
                    "prompt": f"Process this system with {scenario['physics_compliance']:.0%} physics compliance",
                    "task_type": "scientific_analysis",
                    "strategy": "physics_informed",
                    "scientific_result": {
                        "physics_compliance": scenario["physics_compliance"],
                        "physics_violations": scenario["violations"],
                        "symbolic_functions": ["test_function"],
                        "scientific_insights": ["test scenario"],
                        "integrity_score": scenario["physics_compliance"]
                    }
                }
            }
            
            result = agent.process(test_message)
            
            if result["status"] == "success":
                payload = result["payload"]
                routing_results.append({
                    "scenario": scenario["name"],
                    "physics_compliance": scenario["physics_compliance"],
                    "violations_count": len(scenario["violations"]),
                    "final_confidence": payload["confidence"],
                    "providers_used": len(payload["providers_used"]),
                    "strategy_effective": payload["consensus_score"] > 0.6
                })
                
                print(f"   ‚öñÔ∏è {scenario['name']}: {len(payload['providers_used'])} providers, conf: {payload['confidence']:.3f}")
            else:
                print(f"   ‚ùå {scenario['name']}: Failed")
                return False
        
        # Validate routing effectiveness
        high_compliance = next(r for r in routing_results if "High" in r["scenario"])
        low_compliance = next(r for r in routing_results if "Low" in r["scenario"])
        
        # Low compliance should use more providers (validation)
        routing_effective = low_compliance["providers_used"] >= high_compliance["providers_used"]
        
        print(f"   üìä Physics-Informed Routing Analysis:")
        print(f"     ‚Ä¢ High compliance: {high_compliance['providers_used']} providers")
        print(f"     ‚Ä¢ Low compliance: {low_compliance['providers_used']} providers")
        print(f"     ‚Ä¢ Routing effective: {'‚úÖ' if routing_effective else '‚ùå'}")
        
        return routing_effective
        
    except Exception as e:
        print(f"   ‚ùå Physics-informed routing test failed: {e}")
        return False

def test_response_fusion_quality():
    """Test response fusion quality and consensus building."""
    print("\nü§ù Testing Response Fusion Quality...")
    
    try:
        from src.llm.providers.llm_provider_manager import LLMProviderManager, PhysicsInformedContext, TaskType
        
        manager = LLMProviderManager()
        
        # Test fusion with varying provider counts
        fusion_tests = [
            {"providers": 1, "name": "Single Provider"},
            {"providers": 2, "name": "Dual Provider"},
            {"providers": 3, "name": "Triple Provider"},
            {"providers": 4, "name": "Quad Provider"}
        ]
        
        fusion_results = []
        
        for test_config in fusion_tests:
            context = PhysicsInformedContext(
                original_prompt="Compare different approaches to solving this optimization problem",
                physics_compliance=0.74,  # Realistic test value
                symbolic_functions=["minimize f(x) subject to g(x) ‚â§ 0"],
                scientific_insights=["Constrained optimization", "Lagrange multipliers applicable"],
                integrity_score=0.71,  # Realistic test value
                task_type=TaskType.MATHEMATICAL_REASONING
            )
            
            async def test_fusion():
                response = await manager.generate_response(
                    context, 
                    use_fusion=test_config["providers"] > 1, 
                    max_providers=test_config["providers"]
                )
                return response
            
            response = asyncio.run(test_fusion())
            
            # Handle both single and fused responses
            if hasattr(response, 'consensus_score'):
                consensus = response.consensus_score
                providers_used = len(response.contributing_providers)
                confidence = response.confidence
                cost = response.total_cost
            else:
                consensus = 1.0  # Single provider = perfect consensus
                providers_used = 1
                confidence = response.confidence
                cost = response.cost
            
            fusion_results.append({
                "config": test_config["name"],
                "providers_used": providers_used,
                "consensus": consensus,
                "confidence": confidence,
                "cost": cost,
                "cost_per_provider": cost / providers_used
            })
            
            print(f"   ü§ù {test_config['name']}: consensus {consensus:.3f}, conf {confidence:.3f}, cost ${cost:.4f}")
        
        # Analyze fusion quality trends
        print(f"   üìä Fusion Quality Analysis:")
        
        # Check if consensus generally improves with more providers (up to a point)
        consensuses = [r["consensus"] for r in fusion_results if r["providers_used"] > 1]
        if consensuses:
            avg_consensus = np.mean(consensuses)
            print(f"     ‚Ä¢ Average consensus (multi-provider): {avg_consensus:.3f}")
        
        # Check cost efficiency
        cost_efficiencies = [r["confidence"] / r["cost"] for r in fusion_results if r["cost"] > 0]
        if cost_efficiencies:
            best_efficiency = max(cost_efficiencies)
            best_config = fusion_results[cost_efficiencies.index(best_efficiency)]
            print(f"     ‚Ä¢ Best cost efficiency: {best_config['config']} ({best_efficiency:.1f} conf/cost)")
        
        return len(fusion_results) == len(fusion_tests)
        
    except Exception as e:
        print(f"   ‚ùå Response fusion test failed: {e}")
        return False

def test_performance_optimization():
    """Test performance optimization across multiple providers."""
    print("\n‚ö° Testing Performance Optimization...")
    
    try:
        from src.agents.coordination.multi_llm_agent import MultiLLMAgent
        
        agent = MultiLLMAgent()
        
        # Performance test with multiple requests
        test_requests = [
            "Analyze the energy efficiency of this renewable energy system",
            "Validate the thermodynamic consistency of this process",
            "Explore innovative approaches to quantum computing",
            "Coordinate distributed sensor network optimization",
            "Evaluate the stability of this control system"
        ]
        
        performance_results = []
        total_start_time = time.time()
        
        for i, prompt in enumerate(test_requests):
            request_start = time.time()
            
            test_message = {
                "operation": "coordinate",
                "payload": {
                    "prompt": prompt,
                    "task_type": "scientific_analysis",
                    "strategy": "physics_informed",
                    "max_providers": 3,
                    "scientific_result": {
                        "physics_compliance": 0.85 + (i * 0.02),  # Vary compliance
                        "physics_violations": [] if i % 2 == 0 else ["minor issue"],
                        "symbolic_functions": [f"f_{i}(x)"],
                        "scientific_insights": [f"insight_{i}"],
                        "integrity_score": 0.8 + (i * 0.03)
                    }
                }
            }
            
            result = agent.process(test_message)
            request_time = time.time() - request_start
            
            if result["status"] == "success":
                payload = result["payload"]
                performance_results.append({
                    "request": i + 1,
                    "processing_time": request_time,
                    "confidence": payload["confidence"],
                    "consensus": payload["consensus_score"],
                    "providers": len(payload["providers_used"]),
                    "cost": payload.get("total_cost", 0)
                })
                
                print(f"   ‚ö° Request {i+1}: {request_time:.2f}s, conf: {payload['confidence']:.3f}")
            else:
                print(f"   ‚ùå Request {i+1}: Failed")
                return False
        
        total_time = time.time() - total_start_time
        
        # Calculate performance metrics
        avg_time = np.mean([r["processing_time"] for r in performance_results])
        avg_confidence = np.mean([r["confidence"] for r in performance_results])
        avg_consensus = np.mean([r["consensus"] for r in performance_results])
        total_cost = sum(r["cost"] for r in performance_results)
        
        print(f"   üìä Performance Optimization Results:")
        print(f"     ‚Ä¢ Total time: {total_time:.2f}s")
        print(f"     ‚Ä¢ Average per request: {avg_time:.2f}s")
        print(f"     ‚Ä¢ Average confidence: {avg_confidence:.3f}")
        print(f"     ‚Ä¢ Average consensus: {avg_consensus:.3f}")
        print(f"     ‚Ä¢ Total cost: ${total_cost:.4f}")
        print(f"     ‚Ä¢ Requests per minute: {len(test_requests) / (total_time / 60):.1f}")
        
        # Performance targets for Week 4
        targets = {
            "max_avg_time": 15.0,  # Average processing time
            "min_confidence": 0.7,
            "min_consensus": 0.6,
            "max_cost": 0.50  # Total cost for test
        }
        
        meets_targets = (
            avg_time <= targets["max_avg_time"] and
            avg_confidence >= targets["min_confidence"] and
            avg_consensus >= targets["min_consensus"] and
            total_cost <= targets["max_cost"]
        )
        
        print(f"   üéØ Performance targets: {'‚úÖ MET' if meets_targets else '‚ùå NOT MET'}")
        
        return meets_targets
        
    except Exception as e:
        print(f"   ‚ùå Performance optimization test failed: {e}")
        return False

def test_end_to_end_multi_llm_pipeline():
    """Test complete end-to-end multi-LLM pipeline integration."""
    print("\nüîÑ Testing End-to-End Multi-LLM Pipeline...")
    
    try:
        from src.agents.coordination.multi_llm_agent import MultiLLMAgent
        
        agent = MultiLLMAgent()
        
        # Complex multi-stage test
        complex_test = {
            "operation": "coordinate",
            "payload": {
                "prompt": "Design and validate a novel approach to improving energy storage efficiency in quantum dot solar cells, considering both theoretical constraints and practical implementation challenges",
                "task_type": "scientific_analysis",
                "strategy": "physics_informed",
                "validation_level": "rigorous",
                "max_providers": 4,
                "scientific_result": {
                    "physics_compliance": 0.82,
                    "physics_violations": ["quantum efficiency anomaly", "energy gap inconsistency"],
                    "symbolic_functions": [
                        "Œ∑ = (P_out/P_in) * 100%",
                        "V_oc = (kT/q) * ln(I_sc/I_0 + 1)",
                        "E_g = hc/Œª_cutoff"
                    ],
                    "scientific_insights": [
                        "Quantum confinement effects",
                        "Hot carrier extraction",
                        "Multiple exciton generation",
                        "Intermediate band formation"
                    ],
                    "integrity_score": 0.86,
                    "constraint_scores": {
                        "conservation_energy": 0.78,
                        "quantum_mechanics": 0.89,
                        "thermodynamics": 0.85
                    }
                },
                "physics_requirements": {
                    "min_compliance": 0.8,
                    "critical_constraints": ["conservation_energy", "quantum_mechanics"]
                },
                "metadata": {
                    "complexity": "high",
                    "domain": "quantum_photovoltaics",
                    "innovation_required": True
                }
            }
        }
        
        start_time = time.time()
        result = agent.process(complex_test)
        total_time = time.time() - start_time
        
        if result["status"] == "success":
            payload = result["payload"]
            
            print(f"   ‚úÖ End-to-end processing successful")
            print(f"   ‚è±Ô∏è Total processing time: {total_time:.2f}s")
            print(f"   üìä Final confidence: {payload['confidence']:.3f}")
            print(f"   ü§ù Consensus score: {payload['consensus_score']:.3f}")
            print(f"   üß™ Physics compliance: {payload['physics_compliance']:.3f}")
            print(f"   ü§ñ Providers coordinated: {len(payload['providers_used'])}")
            print(f"   üìã Strategy used: {payload['strategy_used']}")
            print(f"   üí∞ Total cost: ${payload['total_cost']:.4f}")
            print(f"   üéØ Recommendations: {len(payload['recommendations'])}")
            
            # Validate end-to-end quality
            quality_checks = {
                "confidence_acceptable": payload["confidence"] > 0.7,
                "consensus_achieved": payload["consensus_score"] > 0.6,
                "physics_addressed": payload["physics_compliance"] <= 0.82,  # Should not increase violations
                "multiple_providers": len(payload["providers_used"]) >= 2,
                "strategy_executed": payload["strategy_used"] == "physics_informed",
                "processing_reasonable": total_time < 30.0,
                "cost_reasonable": payload["total_cost"] < 0.20
            }
            
            passed_checks = sum(quality_checks.values())
            total_checks = len(quality_checks)
            
            print(f"   üîç Quality checks: {passed_checks}/{total_checks}")
            for check, passed in quality_checks.items():
                status = "‚úÖ" if passed else "‚ùå"
                print(f"     {status} {check}")
            
            # Overall pipeline success
            pipeline_success = passed_checks >= total_checks * 0.8  # 80% pass rate
            
            print(f"   üîÑ Pipeline integration: {'‚úÖ SUCCESS' if pipeline_success else '‚ùå NEEDS IMPROVEMENT'}")
            
            return pipeline_success
            
        else:
            print(f"   ‚ùå End-to-end processing failed: {result['payload']}")
            return False
            
    except Exception as e:
        print(f"   ‚ùå End-to-end pipeline test failed: {e}")
        return False

def main():
    """Run comprehensive Week 4 multi-LLM integration testing suite."""
    print("=" * 80)
    print("ü§ñ NIS Protocol V3 - Week 4: Multi-LLM Provider Integration Testing")
    print("=" * 80)
    
    tests = [
        ("LLM Provider Manager", test_llm_provider_manager),
        ("Multi-LLM Agent", test_multi_llm_agent),
        ("Provider Task Specialization", test_provider_task_specialization),
        ("Physics-Informed Routing", test_physics_informed_routing),
        ("Response Fusion Quality", test_response_fusion_quality),
        ("Performance Optimization", test_performance_optimization),
        ("End-to-End Multi-LLM Pipeline", test_end_to_end_multi_llm_pipeline)
    ]
    
    results = []
    total_start_time = time.time()
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        
        try:
            success = test_func()
            results.append((test_name, success))
            
            status = "‚úÖ PASSED" if success else "‚ùå FAILED"
            print(f"\n{test_name}: {status}")
            
        except Exception as e:
            print(f"\n{test_name}: ‚ùå ERROR - {e}")
            results.append((test_name, False))
    
    # Summary
    total_time = time.time() - total_start_time
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    print("\n" + "=" * 80)
    print("üèÅ WEEK 4 MULTI-LLM INTEGRATION TESTING SUMMARY")
    print("=" * 80)
    
    for test_name, success in results:
        status = "‚úÖ PASSED" if success else "‚ùå FAILED"
        print(f"   {test_name:<40} {status}")
    
    print(f"\nüìä Overall Results:")
    print(f"   ‚Ä¢ Tests passed: {passed}/{total}")
    print(f"   ‚Ä¢ Success rate: {passed/total:.1%}")
    print(f"   ‚Ä¢ Total testing time: {total_time:.2f}s")
    
    if passed == total:
        print("\nüéâ ALL TESTS PASSED! Week 4 Multi-LLM Integration is ready!")
        print("   ‚úÖ LLM Provider Manager fully operational")
        print("   ‚úÖ Multi-LLM coordination working")
        print("   ‚úÖ Physics-informed routing active")
        print("   ‚úÖ Response fusion and consensus building functional")
        print("   Ready to proceed to Week 5: Advanced Agent Orchestration")
    else:
        print(f"\n‚ö†Ô∏è {total-passed} tests failed. Review multi-LLM integration before proceeding.")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 