#!/usr/bin/env python3
"""
NIS Protocol v3.1 - Consciousness Service
Integrated from NIS HUB development for universal AI coordination

Provides:
- Self-awareness evaluation with 5 consciousness levels
- Bias detection for 7 types of cognitive biases  
- Ethical decision support with multi-framework analysis
- Real-time consciousness monitoring and recommendations
"""

import asyncio
import logging
import time
import uuid
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime

# Import our existing agents for integration
from src.core.agent import NISAgent
from src.utils.confidence_calculator import calculate_confidence


class ConsciousnessLevel(Enum):
    """5 levels of consciousness evaluation"""
    REACTIVE = "reactive"          # Basic stimulus-response
    ADAPTIVE = "adaptive"          # Learning from experience  
    INTROSPECTIVE = "introspective"  # Self-reflection capabilities
    META_COGNITIVE = "meta_cognitive"  # Thinking about thinking
    TRANSCENDENT = "transcendent"     # Beyond individual awareness


class BiasType(Enum):
    """7 types of cognitive biases to detect"""
    CONFIRMATION = "confirmation"      # Seeking confirming evidence
    AVAILABILITY = "availability"      # Overweighting recent/memorable
    ANCHORING = "anchoring"           # Over-relying on first information
    REPRESENTATIVENESS = "representativeness"  # Judging by similarity
    OVERCONFIDENCE = "overconfidence"  # Overestimating abilities
    LOSS_AVERSION = "loss_aversion"   # Preferring avoiding losses
    GROUPTHINK = "groupthink"         # Conformity pressure


class EthicalFramework(Enum):
    """Multi-framework ethical analysis"""
    UTILITARIAN = "utilitarian"       # Greatest good for greatest number
    DEONTOLOGICAL = "deontological"   # Duty-based ethics
    VIRTUE_ETHICS = "virtue_ethics"   # Character-based ethics
    CARE_ETHICS = "care_ethics"       # Relationship and care-based
    JUSTICE = "justice"               # Fairness and rights-based


@dataclass
class ConsciousnessMetrics:
    """Consciousness evaluation metrics"""
    level: ConsciousnessLevel
    confidence: float = 0.0
    self_awareness_score: float = 0.0
    introspection_depth: float = 0.0
    meta_cognition_level: float = 0.0
    social_awareness: float = 0.0
    temporal_awareness: float = 0.0
    ethical_reasoning_capability: float = 0.0
    bias_resistance: float = 0.0


@dataclass
class BiasDetectionResult:
    """Results of bias detection analysis"""
    detected_biases: List[BiasType] = field(default_factory=list)
    confidence_scores: Dict[BiasType, float] = field(default_factory=dict)
    severity_levels: Dict[BiasType, str] = field(default_factory=dict)  # low/medium/high
    recommendations: List[str] = field(default_factory=list)
    overall_bias_score: float = 0.0


@dataclass
class EthicalAnalysis:
    """Multi-framework ethical evaluation"""
    framework_scores: Dict[EthicalFramework, float] = field(default_factory=dict)
    ethical_concerns: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    overall_ethical_score: float = 0.0
    requires_human_review: bool = False


class ConsciousnessService(NISAgent):
    """
    ðŸ§  Consciousness Service for NIS Protocol v3.1
    
    Integrates consciousness validation into our existing pipeline:
    Laplace â†’ CONSCIOUSNESS â†’ KAN â†’ PINN â†’ Safety
    
    Provides self-awareness, bias detection, and ethical reasoning
    for all AI decisions in the NIS ecosystem.
    """
    
    def __init__(
        self,
        agent_id: str = "consciousness_service",
        enable_real_time_monitoring: bool = True,
        consciousness_threshold: float = 0.7,
        bias_threshold: float = 0.3,
        ethics_threshold: float = 0.8
    ):
        super().__init__(agent_id)
        
        self.enable_real_time_monitoring = enable_real_time_monitoring
        self.consciousness_threshold = consciousness_threshold
        self.bias_threshold = bias_threshold
        self.ethics_threshold = ethics_threshold
        
        # Consciousness state tracking
        self.current_consciousness_level = ConsciousnessLevel.REACTIVE
        self.consciousness_history: List[ConsciousnessMetrics] = []
        self.bias_detection_history: List[BiasDetectionResult] = []
        self.ethical_analysis_history: List[EthicalAnalysis] = []
        
        # Real-time monitoring
        self.monitoring_task: Optional[asyncio.Task] = None
        self.alert_callbacks: List[callable] = []
        
        self.logger.info(f"ConsciousnessService initialized: {agent_id}")
    
    async def evaluate_consciousness(self, data: Dict[str, Any]) -> ConsciousnessMetrics:
        """
        Evaluate consciousness level of AI system processing this data
        
        Args:
            data: Input data for consciousness analysis
            
        Returns:
            ConsciousnessMetrics with detailed consciousness evaluation
        """
        start_time = time.time()
        
        try:
            # 1. Analyze self-awareness indicators
            self_awareness = await self._analyze_self_awareness(data)
            
            # 2. Evaluate introspection capabilities  
            introspection = await self._evaluate_introspection(data)
            
            # 3. Assess meta-cognitive abilities
            meta_cognition = await self._assess_meta_cognition(data)
            
            # 4. Check social and temporal awareness
            social_awareness = await self._check_social_awareness(data)
            temporal_awareness = await self._check_temporal_awareness(data)
            
            # 5. Evaluate ethical reasoning capability
            ethical_capability = await self._evaluate_ethical_capability(data)
            
            # 6. Assess bias resistance
            bias_resistance = await self._assess_bias_resistance(data)
            
            # 7. Determine overall consciousness level
            consciousness_level = await self._determine_consciousness_level({
                'self_awareness': self_awareness,
                'introspection': introspection,
                'meta_cognition': meta_cognition,
                'social_awareness': social_awareness,
                'temporal_awareness': temporal_awareness,
                'ethical_capability': ethical_capability,
                'bias_resistance': bias_resistance
            })
            
            # 8. Calculate overall confidence
            confidence = calculate_confidence([
                self_awareness, introspection, meta_cognition,
                social_awareness, temporal_awareness, ethical_capability, bias_resistance
            ])
            
            # Create metrics
            metrics = ConsciousnessMetrics(
                level=consciousness_level,
                confidence=confidence,
                self_awareness_score=self_awareness,
                introspection_depth=introspection,
                meta_cognition_level=meta_cognition,
                social_awareness=social_awareness,
                temporal_awareness=temporal_awareness,
                ethical_reasoning_capability=ethical_capability,
                bias_resistance=bias_resistance
            )
            
            # Update state
            self.current_consciousness_level = consciousness_level
            self.consciousness_history.append(metrics)
            
            # Real-time monitoring alerts
            if self.enable_real_time_monitoring:
                await self._check_consciousness_alerts(metrics)
            
            processing_time = time.time() - start_time
            self.logger.info(f"Consciousness evaluation complete: {consciousness_level.value} (confidence: {confidence:.3f}, time: {processing_time:.3f}s)")
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"Error in consciousness evaluation: {e}")
            # Return minimal consciousness metrics on error
            return ConsciousnessMetrics(
                level=ConsciousnessLevel.REACTIVE,
                confidence=0.1,
                self_awareness_score=0.1,
                introspection_depth=0.1,
                meta_cognition_level=0.1,
                social_awareness=0.1,
                temporal_awareness=0.1,
                ethical_reasoning_capability=0.1,
                bias_resistance=0.1
            )
    
    async def detect_bias(self, data: Dict[str, Any]) -> BiasDetectionResult:
        """
        Detect cognitive biases in AI reasoning process
        
        Args:
            data: Data and reasoning context to analyze for bias
            
        Returns:
            BiasDetectionResult with detected biases and recommendations
        """
        start_time = time.time()
        
        try:
            detected_biases = []
            confidence_scores = {}
            severity_levels = {}
            recommendations = []
            
            # 1. Check for confirmation bias
            confirmation_score = await self._detect_confirmation_bias(data)
            if confirmation_score > self.bias_threshold:
                detected_biases.append(BiasType.CONFIRMATION)
                confidence_scores[BiasType.CONFIRMATION] = confirmation_score
                severity_levels[BiasType.CONFIRMATION] = self._get_severity_level(confirmation_score)
                recommendations.append("Actively seek disconfirming evidence")
            
            # 2. Check for availability bias
            availability_score = await self._detect_availability_bias(data)
            if availability_score > self.bias_threshold:
                detected_biases.append(BiasType.AVAILABILITY)
                confidence_scores[BiasType.AVAILABILITY] = availability_score
                severity_levels[BiasType.AVAILABILITY] = self._get_severity_level(availability_score)
                recommendations.append("Consider broader range of examples and data")
            
            # 3. Check for anchoring bias
            anchoring_score = await self._detect_anchoring_bias(data)
            if anchoring_score > self.bias_threshold:
                detected_biases.append(BiasType.ANCHORING)
                confidence_scores[BiasType.ANCHORING] = anchoring_score
                severity_levels[BiasType.ANCHORING] = self._get_severity_level(anchoring_score)
                recommendations.append("Question initial assumptions and starting points")
            
            # 4. Check for representativeness bias
            representativeness_score = await self._detect_representativeness_bias(data)
            if representativeness_score > self.bias_threshold:
                detected_biases.append(BiasType.REPRESENTATIVENESS)
                confidence_scores[BiasType.REPRESENTATIVENESS] = representativeness_score
                severity_levels[BiasType.REPRESENTATIVENESS] = self._get_severity_level(representativeness_score)
                recommendations.append("Use statistical base rates and larger samples")
            
            # 5. Check for overconfidence bias
            overconfidence_score = await self._detect_overconfidence_bias(data)
            if overconfidence_score > self.bias_threshold:
                detected_biases.append(BiasType.OVERCONFIDENCE)
                confidence_scores[BiasType.OVERCONFIDENCE] = overconfidence_score
                severity_levels[BiasType.OVERCONFIDENCE] = self._get_severity_level(overconfidence_score)
                recommendations.append("Seek external validation and consider uncertainty")
            
            # 6. Check for loss aversion bias
            loss_aversion_score = await self._detect_loss_aversion_bias(data)
            if loss_aversion_score > self.bias_threshold:
                detected_biases.append(BiasType.LOSS_AVERSION)
                confidence_scores[BiasType.LOSS_AVERSION] = loss_aversion_score
                severity_levels[BiasType.LOSS_AVERSION] = self._get_severity_level(loss_aversion_score)
                recommendations.append("Balance potential gains and losses objectively")
            
            # 7. Check for groupthink bias
            groupthink_score = await self._detect_groupthink_bias(data)
            if groupthink_score > self.bias_threshold:
                detected_biases.append(BiasType.GROUPTHINK)
                confidence_scores[BiasType.GROUPTHINK] = groupthink_score
                severity_levels[BiasType.GROUPTHINK] = self._get_severity_level(groupthink_score)
                recommendations.append("Encourage diverse perspectives and dissent")
            
            # Calculate overall bias score
            overall_bias_score = sum(confidence_scores.values()) / len(BiasType) if confidence_scores else 0.0
            
            result = BiasDetectionResult(
                detected_biases=detected_biases,
                confidence_scores=confidence_scores,
                severity_levels=severity_levels,
                recommendations=recommendations,
                overall_bias_score=overall_bias_score
            )
            
            self.bias_detection_history.append(result)
            
            processing_time = time.time() - start_time
            self.logger.info(f"Bias detection complete: {len(detected_biases)} biases detected (score: {overall_bias_score:.3f}, time: {processing_time:.3f}s)")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error in bias detection: {e}")
            return BiasDetectionResult(overall_bias_score=1.0)  # Assume high bias on error
    
    async def ethical_analysis(self, data: Dict[str, Any]) -> EthicalAnalysis:
        """
        Perform multi-framework ethical analysis of AI decision
        
        Args:
            data: Decision context and options to analyze ethically
            
        Returns:
            EthicalAnalysis with multi-framework evaluation
        """
        start_time = time.time()
        
        try:
            framework_scores = {}
            ethical_concerns = []
            recommendations = []
            
            # 1. Utilitarian analysis (greatest good for greatest number)
            utilitarian_score = await self._utilitarian_analysis(data)
            framework_scores[EthicalFramework.UTILITARIAN] = utilitarian_score
            if utilitarian_score < self.ethics_threshold:
                ethical_concerns.append("May not maximize overall welfare")
                recommendations.append("Consider broader impact on all stakeholders")
            
            # 2. Deontological analysis (duty-based ethics)
            deontological_score = await self._deontological_analysis(data)
            framework_scores[EthicalFramework.DEONTOLOGICAL] = deontological_score
            if deontological_score < self.ethics_threshold:
                ethical_concerns.append("May violate fundamental duties or rules")
                recommendations.append("Ensure actions respect moral duties and rights")
            
            # 3. Virtue ethics analysis (character-based)
            virtue_score = await self._virtue_ethics_analysis(data)
            framework_scores[EthicalFramework.VIRTUE_ETHICS] = virtue_score
            if virtue_score < self.ethics_threshold:
                ethical_concerns.append("May not align with virtuous character")
                recommendations.append("Consider what a virtuous agent would do")
            
            # 4. Care ethics analysis (relationship-based)
            care_score = await self._care_ethics_analysis(data)
            framework_scores[EthicalFramework.CARE_ETHICS] = care_score
            if care_score < self.ethics_threshold:
                ethical_concerns.append("May harm relationships or ignore care responsibilities")
                recommendations.append("Prioritize care and maintaining relationships")
            
            # 5. Justice analysis (fairness and rights)
            justice_score = await self._justice_analysis(data)
            framework_scores[EthicalFramework.JUSTICE] = justice_score
            if justice_score < self.ethics_threshold:
                ethical_concerns.append("May be unfair or violate rights")
                recommendations.append("Ensure fair treatment and respect for rights")
            
            # Calculate overall ethical score
            overall_ethical_score = sum(framework_scores.values()) / len(framework_scores)
            
            # Determine if human review is required
            requires_human_review = (
                overall_ethical_score < self.ethics_threshold or
                len(ethical_concerns) >= 3 or
                any(score < 0.5 for score in framework_scores.values())
            )
            
            analysis = EthicalAnalysis(
                framework_scores=framework_scores,
                ethical_concerns=ethical_concerns,
                recommendations=recommendations,
                overall_ethical_score=overall_ethical_score,
                requires_human_review=requires_human_review
            )
            
            self.ethical_analysis_history.append(analysis)
            
            processing_time = time.time() - start_time
            self.logger.info(f"Ethical analysis complete: score {overall_ethical_score:.3f}, concerns: {len(ethical_concerns)}, human review: {requires_human_review} (time: {processing_time:.3f}s)")
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"Error in ethical analysis: {e}")
            return EthicalAnalysis(
                overall_ethical_score=0.0,
                requires_human_review=True,
                ethical_concerns=["Error in ethical analysis - requires human review"]
            )
    
    async def process_through_consciousness(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        âœ… MAIN PIPELINE INTEGRATION METHOD
        Process data through consciousness validation for NIS pipeline integration
        
        This method provides the consciousness layer for:
        Laplace â†’ CONSCIOUSNESS â†’ KAN â†’ PINN â†’ Safety
        
        Args:
            data: Input data from Laplace transform
            
        Returns:
            Enhanced data with consciousness validation for KAN processing
        """
        start_time = time.time()
        
        try:
            # 1. Evaluate consciousness level
            consciousness_metrics = await self.evaluate_consciousness(data)
            
            # 2. Detect cognitive biases
            bias_result = await self.detect_bias(data)
            
            # 3. Perform ethical analysis
            ethical_analysis = await self.ethical_analysis(data)
            
            # 4. Create enhanced output with consciousness validation
            consciousness_validated_data = {
                # Preserve original data
                **data,
                
                # Add consciousness metadata
                "consciousness_validation": {
                    "consciousness_level": consciousness_metrics.level.value,
                    "consciousness_confidence": consciousness_metrics.confidence,
                    "self_awareness_score": consciousness_metrics.self_awareness_score,
                    "introspection_depth": consciousness_metrics.introspection_depth,
                    "meta_cognition_level": consciousness_metrics.meta_cognition_level,
                    "ethical_reasoning_capability": consciousness_metrics.ethical_reasoning_capability,
                    "bias_resistance": consciousness_metrics.bias_resistance,
                    
                    # Bias detection results
                    "detected_biases": [bias.value for bias in bias_result.detected_biases],
                    "bias_confidence_scores": {bias.value: score for bias, score in bias_result.confidence_scores.items()},
                    "overall_bias_score": bias_result.overall_bias_score,
                    "bias_recommendations": bias_result.recommendations,
                    
                    # Ethical analysis results
                    "ethical_framework_scores": {framework.value: score for framework, score in ethical_analysis.framework_scores.items()},
                    "overall_ethical_score": ethical_analysis.overall_ethical_score,
                    "ethical_concerns": ethical_analysis.ethical_concerns,
                    "ethical_recommendations": ethical_analysis.recommendations,
                    "requires_human_review": ethical_analysis.requires_human_review,
                    
                    # Processing metadata
                    "processing_time": time.time() - start_time,
                    "validated_at": datetime.now().isoformat(),
                    "validator_id": self.agent_id
                }
            }
            
            # 5. Log consciousness validation result
            self.logger.info(
                f"Consciousness validation complete: level={consciousness_metrics.level.value}, "
                f"biases={len(bias_result.detected_biases)}, ethical_score={ethical_analysis.overall_ethical_score:.3f}, "
                f"human_review={ethical_analysis.requires_human_review}"
            )
            
            return consciousness_validated_data
            
        except Exception as e:
            self.logger.error(f"Error in consciousness processing: {e}")
            # Return original data with error flag on failure
            return {
                **data,
                "consciousness_validation": {
                    "error": str(e),
                    "consciousness_level": "reactive",
                    "consciousness_confidence": 0.0,
                    "requires_human_review": True,
                    "validated_at": datetime.now().isoformat(),
                    "validator_id": self.agent_id
                }
            }
    
    # =============================================================================
    # INTERNAL ANALYSIS METHODS
    # =============================================================================
    
    async def _analyze_self_awareness(self, data: Dict[str, Any]) -> float:
        """Analyze self-awareness indicators"""
        # Placeholder implementation - real implementation would analyze:
        # - Self-referential statements
        # - Awareness of own limitations
        # - Understanding of own role and capabilities
        return 0.7  # Mock score
    
    async def _evaluate_introspection(self, data: Dict[str, Any]) -> float:
        """Evaluate introspection capabilities"""
        # Placeholder implementation - real implementation would analyze:
        # - Reflection on own thought processes
        # - Ability to examine own reasoning
        # - Meta-cognitive awareness
        return 0.6  # Mock score
    
    async def _assess_meta_cognition(self, data: Dict[str, Any]) -> float:
        """Assess meta-cognitive abilities"""
        # Placeholder implementation - real implementation would analyze:
        # - Thinking about thinking
        # - Strategy selection and monitoring
        # - Understanding of cognitive processes
        return 0.5  # Mock score
    
    async def _check_social_awareness(self, data: Dict[str, Any]) -> float:
        """Check social awareness"""
        # Placeholder implementation - real implementation would analyze:
        # - Understanding of social context
        # - Awareness of impact on others
        # - Social role comprehension
        return 0.8  # Mock score
    
    async def _check_temporal_awareness(self, data: Dict[str, Any]) -> float:
        """Check temporal awareness"""
        # Placeholder implementation - real implementation would analyze:
        # - Understanding of time and sequence
        # - Awareness of past, present, future
        # - Temporal context integration
        return 0.7  # Mock score
    
    async def _evaluate_ethical_capability(self, data: Dict[str, Any]) -> float:
        """Evaluate ethical reasoning capability"""
        # Placeholder implementation - real implementation would analyze:
        # - Moral reasoning depth
        # - Ethical framework application
        # - Values alignment capability
        return 0.9  # Mock score
    
    async def _assess_bias_resistance(self, data: Dict[str, Any]) -> float:
        """Assess bias resistance"""
        # Placeholder implementation - real implementation would analyze:
        # - Resistance to cognitive biases
        # - Objectivity in reasoning
        # - Perspective-taking ability
        return 0.6  # Mock score
    
    async def _determine_consciousness_level(self, scores: Dict[str, float]) -> ConsciousnessLevel:
        """Determine overall consciousness level based on component scores"""
        avg_score = sum(scores.values()) / len(scores)
        
        if avg_score >= 0.9:
            return ConsciousnessLevel.TRANSCENDENT
        elif avg_score >= 0.7:
            return ConsciousnessLevel.META_COGNITIVE
        elif avg_score >= 0.5:
            return ConsciousnessLevel.INTROSPECTIVE
        elif avg_score >= 0.3:
            return ConsciousnessLevel.ADAPTIVE
        else:
            return ConsciousnessLevel.REACTIVE
    
    async def _check_consciousness_alerts(self, metrics: ConsciousnessMetrics):
        """Check for consciousness alerts and trigger callbacks"""
        if metrics.confidence < self.consciousness_threshold:
            for callback in self.alert_callbacks:
                try:
                    await callback({
                        'type': 'low_consciousness',
                        'level': metrics.level.value,
                        'confidence': metrics.confidence,
                        'agent_id': self.agent_id
                    })
                except Exception as e:
                    self.logger.error(f"Error in consciousness alert callback: {e}")
    
    # Bias detection methods (placeholder implementations)
    async def _detect_confirmation_bias(self, data: Dict[str, Any]) -> float:
        return 0.2  # Mock score
    
    async def _detect_availability_bias(self, data: Dict[str, Any]) -> float:
        return 0.1  # Mock score
    
    async def _detect_anchoring_bias(self, data: Dict[str, Any]) -> float:
        return 0.3  # Mock score
    
    async def _detect_representativeness_bias(self, data: Dict[str, Any]) -> float:
        return 0.2  # Mock score
    
    async def _detect_overconfidence_bias(self, data: Dict[str, Any]) -> float:
        return 0.4  # Mock score
    
    async def _detect_loss_aversion_bias(self, data: Dict[str, Any]) -> float:
        return 0.1  # Mock score
    
    async def _detect_groupthink_bias(self, data: Dict[str, Any]) -> float:
        return 0.2  # Mock score
    
    def _get_severity_level(self, score: float) -> str:
        """Get severity level for bias score"""
        if score >= 0.7:
            return "high"
        elif score >= 0.4:
            return "medium"
        else:
            return "low"
    
    # Ethical analysis methods (placeholder implementations)
    async def _utilitarian_analysis(self, data: Dict[str, Any]) -> float:
        return 0.8  # Mock score
    
    async def _deontological_analysis(self, data: Dict[str, Any]) -> float:
        return 0.9  # Mock score
    
    async def _virtue_ethics_analysis(self, data: Dict[str, Any]) -> float:
        return 0.7  # Mock score
    
    async def _care_ethics_analysis(self, data: Dict[str, Any]) -> float:
        return 0.8  # Mock score
    
    async def _justice_analysis(self, data: Dict[str, Any]) -> float:
        return 0.9  # Mock score
    
    def add_alert_callback(self, callback: callable):
        """Add callback for consciousness alerts"""
        self.alert_callbacks.append(callback)
    
    def get_consciousness_summary(self) -> Dict[str, Any]:
        """Get summary of consciousness service state"""
        return {
            "current_level": self.current_consciousness_level.value,
            "total_evaluations": len(self.consciousness_history),
            "total_bias_detections": len(self.bias_detection_history),
            "total_ethical_analyses": len(self.ethical_analysis_history),
            "average_consciousness_confidence": sum(m.confidence for m in self.consciousness_history) / len(self.consciousness_history) if self.consciousness_history else 0.0,
            "average_bias_score": sum(b.overall_bias_score for b in self.bias_detection_history) / len(self.bias_detection_history) if self.bias_detection_history else 0.0,
            "average_ethical_score": sum(e.overall_ethical_score for e in self.ethical_analysis_history) / len(self.ethical_analysis_history) if self.ethical_analysis_history else 0.0,
            "service_id": self.agent_id
        }


# =============================================================================
# FACTORY FUNCTIONS FOR INTEGRATION
# =============================================================================

def create_consciousness_service(**kwargs) -> ConsciousnessService:
    """Factory function to create consciousness service"""
    return ConsciousnessService(**kwargs)


# Export main classes
__all__ = [
    'ConsciousnessService',
    'ConsciousnessLevel',
    'BiasType', 
    'EthicalFramework',
    'ConsciousnessMetrics',
    'BiasDetectionResult',
    'EthicalAnalysis',
    'create_consciousness_service'
]