"""
🧠 NIS Protocol v3.2 - Enhanced Reasoning Chain
Multi-model collaborative reasoning with chain-of-thought and metacognitive capabilities
"""

import asyncio
import logging
from typing import Dict, List, Optional, Union, Any, Tuple
from datetime import datetime
import json
from enum import Enum

from src.core.agent import NISAgent
from src.llm.llm_manager import GeneralLLMProvider

logger = logging.getLogger(__name__)

class ReasoningType(Enum):
    """Types of reasoning tasks"""
    MATHEMATICAL = "mathematical"
    LOGICAL = "logical"
    CREATIVE = "creative"
    ANALYTICAL = "analytical"
    SCIENTIFIC = "scientific"
    ETHICAL = "ethical"
    STRATEGIC = "strategic"
    CAUSAL = "causal"
    PHILOSOPHICAL = "philosophical"

class ModelSpecialization(Enum):
    """Model specializations for different reasoning types"""
    CLAUDE_OPUS = "claude-3-opus"           # Best for mathematical and analytical
    CLAUDE_SONNET = "claude-3-5-sonnet"    # Best for code and technical analysis
    GPT4_TURBO = "gpt-4-turbo"             # Best for creative and general reasoning
    DEEPSEEK = "deepseek-chat"             # Best for scientific and research
    GOOGLE_GEMINI = "gemini-pro"           # Best for multimodal and factual

class ReasoningStage(Enum):
    """Stages in the reasoning chain"""
    PROBLEM_ANALYSIS = "problem_analysis"
    HYPOTHESIS_GENERATION = "hypothesis_generation"
    EVIDENCE_GATHERING = "evidence_gathering"
    CRITICAL_EVALUATION = "critical_evaluation"
    SYNTHESIS = "synthesis"
    VALIDATION = "validation"
    METACOGNITION = "metacognition"

class EnhancedReasoningChain(NISAgent):
    """
    🧠 Advanced Multi-Model Collaborative Reasoning System
    
    Capabilities:
    - Chain-of-thought reasoning across multiple models
    - Model specialization for different problem types
    - Cross-validation and error checking
    - Metacognitive reasoning about reasoning quality
    - Collaborative problem solving with debate/consensus
    - Uncertainty quantification and confidence tracking
    """
    
    def __init__(self, agent_id: str = "enhanced_reasoning_chain"):
        super().__init__(agent_id)
        self.llm_provider = None
        try:
            self.llm_provider = GeneralLLMProvider()
            logger.info("🧠 EnhancedReasoningChain initialized with active LLM provider")
        except Exception as e:
            logger.warning(f"LLM provider unavailable for EnhancedReasoningChain: {e}. Falling back to heuristic reasoning.")
        
        # Model specialization mapping
        self.model_specializations = {
            ReasoningType.MATHEMATICAL: [ModelSpecialization.CLAUDE_OPUS, ModelSpecialization.DEEPSEEK],
            ReasoningType.LOGICAL: [ModelSpecialization.CLAUDE_OPUS, ModelSpecialization.GPT4_TURBO],
            ReasoningType.CREATIVE: [ModelSpecialization.GPT4_TURBO, ModelSpecialization.CLAUDE_SONNET],
            ReasoningType.ANALYTICAL: [ModelSpecialization.CLAUDE_OPUS, ModelSpecialization.DEEPSEEK],
            ReasoningType.SCIENTIFIC: [ModelSpecialization.DEEPSEEK, ModelSpecialization.GOOGLE_GEMINI],
            ReasoningType.ETHICAL: [ModelSpecialization.CLAUDE_OPUS, ModelSpecialization.GPT4_TURBO],
            ReasoningType.STRATEGIC: [ModelSpecialization.GPT4_TURBO, ModelSpecialization.CLAUDE_SONNET],
            ReasoningType.CAUSAL: [ModelSpecialization.CLAUDE_OPUS, ModelSpecialization.DEEPSEEK],
            ReasoningType.PHILOSOPHICAL: [ModelSpecialization.CLAUDE_OPUS, ModelSpecialization.GPT4_TURBO]
        }
        
        # Reasoning chain templates
        self.reasoning_stages = [
            ReasoningStage.PROBLEM_ANALYSIS,
            ReasoningStage.HYPOTHESIS_GENERATION,
            ReasoningStage.EVIDENCE_GATHERING,
            ReasoningStage.CRITICAL_EVALUATION,
            ReasoningStage.SYNTHESIS,
            ReasoningStage.VALIDATION,
            ReasoningStage.METACOGNITION
        ]
        
        # Confidence tracking
        self.confidence_weights = {
            ModelSpecialization.CLAUDE_OPUS: 0.95,
            ModelSpecialization.CLAUDE_SONNET: 0.90,
            ModelSpecialization.GPT4_TURBO: 0.90,
            ModelSpecialization.DEEPSEEK: 0.85,
            ModelSpecialization.GOOGLE_GEMINI: 0.80
        }
        
    async def collaborative_reasoning(
        self,
        problem: str,
        reasoning_type: ReasoningType = None,
        depth: str = "comprehensive",
        require_consensus: bool = True,
        max_iterations: int = 3
    ) -> Dict[str, Any]:
        """
        🎯 Perform collaborative reasoning with multiple models
        
        Args:
            problem: The problem or question to reason about
            reasoning_type: Type of reasoning required (auto-detected if None)
            depth: Reasoning depth (basic, comprehensive, exhaustive)
            require_consensus: Whether to require consensus between models
            max_iterations: Maximum reasoning iterations
            
        Returns:
            Comprehensive reasoning results with confidence scores
        """
        if not self.llm_provider:
            return self._generate_offline_reasoning(problem, reasoning_type)

        try:
            reasoning_start = datetime.now()
            
            # Auto-detect reasoning type if not specified
            if reasoning_type is None:
                reasoning_type = await self._detect_reasoning_type(problem)
            
            # Get optimal models for this reasoning type
            optimal_models = self.model_specializations.get(
                reasoning_type, 
                [ModelSpecialization.CLAUDE_OPUS, ModelSpecialization.GPT4_TURBO]
            )
            
            # Initialize reasoning chain
            reasoning_chain = []
            
            # Stage 1: Problem Analysis (parallel with multiple models)
            analysis_results = await self._parallel_problem_analysis(problem, optimal_models)
            reasoning_chain.append({
                "stage": ReasoningStage.PROBLEM_ANALYSIS,
                "results": analysis_results,
                "timestamp": self._get_timestamp()
            })
            
            # Stage 2: Hypothesis Generation (collaborative)
            hypotheses = await self._collaborative_hypothesis_generation(
                problem, analysis_results, optimal_models
            )
            reasoning_chain.append({
                "stage": ReasoningStage.HYPOTHESIS_GENERATION,
                "results": hypotheses,
                "timestamp": self._get_timestamp()
            })
            
            # Stage 3: Evidence Gathering (distributed)
            evidence = await self._distributed_evidence_gathering(
                problem, hypotheses, optimal_models
            )
            reasoning_chain.append({
                "stage": ReasoningStage.EVIDENCE_GATHERING,
                "results": evidence,
                "timestamp": self._get_timestamp()
            })
            
            # Stage 4: Critical Evaluation (cross-validation)
            evaluation = await self._cross_model_evaluation(
                problem, hypotheses, evidence, optimal_models
            )
            reasoning_chain.append({
                "stage": ReasoningStage.CRITICAL_EVALUATION,
                "results": evaluation,
                "timestamp": self._get_timestamp()
            })
            
            # Stage 5: Synthesis (consensus building)
            synthesis = await self._consensus_synthesis(
                problem, reasoning_chain, optimal_models, require_consensus
            )
            reasoning_chain.append({
                "stage": ReasoningStage.SYNTHESIS,
                "results": synthesis,
                "timestamp": self._get_timestamp()
            })
            
            # Stage 6: Validation (final cross-check)
            validation = await self._final_validation(
                problem, synthesis, optimal_models
            )
            reasoning_chain.append({
                "stage": ReasoningStage.VALIDATION,
                "results": validation,
                "timestamp": self._get_timestamp()
            })
            
            # Stage 7: Metacognitive Analysis
            metacognition = await self._metacognitive_analysis(
                problem, reasoning_chain, optimal_models
            )
            reasoning_chain.append({
                "stage": ReasoningStage.METACOGNITION,
                "results": metacognition,
                "timestamp": self._get_timestamp()
            })
            
            # Calculate final confidence and quality scores
            final_confidence = self._calculate_reasoning_confidence(reasoning_chain)
            quality_metrics = self._calculate_quality_metrics(reasoning_chain)
            
            reasoning_time = (datetime.now() - reasoning_start).total_seconds()
            
            return {
                "status": "success",
                "problem": problem,
                "reasoning_type": reasoning_type.value,
                "reasoning_chain": reasoning_chain,
                "final_answer": synthesis.get("consensus_answer", "No consensus reached"),
                "confidence": final_confidence,
                "quality_metrics": quality_metrics,
                "models_used": [model.value for model in optimal_models],
                "reasoning_time": reasoning_time,
                "consensus_achieved": synthesis.get("consensus_achieved", False),
                "alternative_solutions": synthesis.get("alternative_solutions", []),
                "limitations": metacognition.get("identified_limitations", []),
                "timestamp": self._get_timestamp()
            }
            
        except Exception as e:
            logger.error(f"Collaborative reasoning failed: {e}")
            return self._generate_offline_reasoning(problem, reasoning_type)
    
    async def debate_reasoning(
        self,
        problem: str,
        positions: List[str] = None,
        rounds: int = 3
    ) -> Dict[str, Any]:
        """
        🗣️ Conduct structured debate between models to reach better conclusions
        
        Args:
            problem: Problem to debate
            positions: Initial positions (auto-generated if None)
            rounds: Number of debate rounds
            
        Returns:
            Debate results with final consensus or disagreement analysis
        """
        if not self.llm_provider:
            return self._generate_offline_debate(problem, positions, rounds)

        try:
            # Auto-generate initial positions if not provided
            if positions is None:
                positions = await self._generate_debate_positions(problem)
            
            debate_history = []
            
            for round_num in range(rounds):
                round_results = await self._conduct_debate_round(
                    problem, positions, round_num, debate_history
                )
                debate_history.append(round_results)
                
                # Update positions based on arguments
                positions = await self._update_positions(positions, round_results)
            
            # Final synthesis and consensus attempt
            final_synthesis = await self._synthesize_debate(problem, debate_history)
            
            return {
                "status": "success",
                "problem": problem,
                "debate_rounds": len(debate_history),
                "debate_history": debate_history,
                "final_synthesis": final_synthesis,
                "consensus_reached": final_synthesis.get("consensus", False),
                "winning_arguments": final_synthesis.get("strongest_arguments", []),
                "timestamp": self._get_timestamp()
            }
            
        except Exception as e:
            logger.error(f"Debate reasoning failed: {e}")
            return self._generate_offline_debate(problem, positions, rounds)
    
    async def _detect_reasoning_type(self, problem: str) -> ReasoningType:
        """Auto-detect the type of reasoning required"""
        # Use LLM to classify the problem type
        classification_prompt = f"""
        Analyze this problem and classify the primary type of reasoning required:
        
        Problem: {problem}
        
        Choose from: mathematical, logical, creative, analytical, scientific, ethical, strategic, causal
        
        Respond with just the classification type.
        """
        
        try:
            result = await self.llm_provider.generate_response([
                {"role": "system", "content": "You are an expert at classifying reasoning problems."},
                {"role": "user", "content": classification_prompt}
            ])
            
            classification = result.get("content", "analytical").lower().strip()
            
            # Map to enum
            for reasoning_type in ReasoningType:
                if reasoning_type.value in classification:
                    return reasoning_type
                    
        except Exception as e:
            logger.warning(f"Reasoning type detection failed: {e}")
        
        # Default fallback
        return ReasoningType.ANALYTICAL
    
    async def _parallel_problem_analysis(
        self, 
        problem: str, 
        models: List[ModelSpecialization]
    ) -> Dict[str, Any]:
        """Analyze problem from multiple model perspectives simultaneously"""
        
        analysis_prompt = f"""
        Analyze this problem from your specialized perspective:
        
        Problem: {problem}
        
        Provide:
        1. Problem decomposition
        2. Key challenges identified
        3. Required knowledge domains
        4. Potential solution approaches
        5. Confidence in your analysis (0-1)
        
        Be specific and detailed in your analysis.
        """
        
        # Run analysis in parallel across models
        tasks = []
        for model in models:
            task = self._get_model_analysis(problem, analysis_prompt, model)
            tasks.append(task)
        
        analyses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Combine and structure results
        combined_analysis = {
            "individual_analyses": {},
            "common_themes": [],
            "divergent_perspectives": [],
            "confidence_scores": {}
        }
        
        for i, analysis in enumerate(analyses):
            if isinstance(analysis, dict):
                model_name = models[i].value
                combined_analysis["individual_analyses"][model_name] = analysis
                combined_analysis["confidence_scores"][model_name] = analysis.get("confidence", 0.5)
        
        # Extract common themes and divergent perspectives
        combined_analysis["common_themes"] = await self._extract_common_themes(
            combined_analysis["individual_analyses"]
        )
        combined_analysis["divergent_perspectives"] = await self._extract_divergent_perspectives(
            combined_analysis["individual_analyses"]
        )
        
        return combined_analysis
    
    async def _get_model_analysis(
        self, 
        problem: str, 
        prompt: str, 
        model: ModelSpecialization
    ) -> Dict[str, Any]:
        """Get analysis from a specific model"""
        if self.llm_provider:
            try:
                result = await self.llm_provider.generate_response(
                    [{"role": "user", "content": prompt}],
                    requested_provider=self._map_model_to_provider(model)
                )

                if result:
                    return {
                        "analysis": result.get("content", ""),
                        "confidence": result.get("confidence", 0.5),
                        "model": model.value,
                        "timestamp": self._get_timestamp()
                    }

            except Exception as e:
                logger.error(f"Model analysis failed for {model.value}: {e}")

        # Fallback heuristic analysis when LLM unavailable or fails
        return self._generate_offline_analysis(problem, model)

    def _generate_offline_analysis(self, problem: str, model: ModelSpecialization) -> Dict[str, Any]:
        """Generate heuristic analysis when LLM provider is unavailable"""

        heuristics = {
            ModelSpecialization.CLAUDE_OPUS: "Mathematical and analytical breakdown",
            ModelSpecialization.CLAUDE_SONNET: "Technical reasoning with code insights",
            ModelSpecialization.GPT4_TURBO: "Creative and broad perspective analysis",
            ModelSpecialization.DEEPSEEK: "Scientific and research-focused perspective",
            ModelSpecialization.GOOGLE_GEMINI: "Multimodal factual synthesis"
        }

        analysis = f"Offline analysis for '{problem}'. Focus: {heuristics.get(model, 'General reasoning')}"

        return {
            "analysis": analysis,
            "confidence": 0.45,
            "model": model.value,
            "method": "heuristic_offline",
            "timestamp": self._get_timestamp()
        }
    
    def _map_model_to_provider(self, model: ModelSpecialization) -> str:
        """Map model specialization to provider name"""
        mapping = {
            ModelSpecialization.CLAUDE_OPUS: "anthropic",
            ModelSpecialization.CLAUDE_SONNET: "anthropic",
            ModelSpecialization.GPT4_TURBO: "openai",
            ModelSpecialization.DEEPSEEK: "deepseek",
            ModelSpecialization.GOOGLE_GEMINI: "google"
        }
        return mapping.get(model, "deepseek")
    
    async def _collaborative_hypothesis_generation(
        self, 
        problem: str, 
        analysis_results: Dict, 
        models: List[ModelSpecialization]
    ) -> Dict[str, Any]:
        """Generate hypotheses collaboratively"""
        # This would implement collaborative hypothesis generation
        return {
            "hypotheses": [
                "Hypothesis 1: Based on analysis...",
                "Hypothesis 2: Alternative approach...",
                "Hypothesis 3: Creative solution..."
            ],
            "generation_method": "collaborative",
            "confidence": 0.8
        }
    
    async def _distributed_evidence_gathering(
        self, 
        problem: str, 
        hypotheses: Dict, 
        models: List[ModelSpecialization]
    ) -> Dict[str, Any]:
        """Gather evidence for hypotheses across models"""
        # This would implement distributed evidence gathering
        return {
            "evidence": {
                "supporting": ["Evidence point 1", "Evidence point 2"],
                "contradicting": ["Counter-evidence 1"],
                "neutral": ["Neutral observation 1"]
            },
            "sources": ["Source 1", "Source 2"],
            "confidence": 0.75
        }
    
    async def _cross_model_evaluation(
        self, 
        problem: str, 
        hypotheses: Dict, 
        evidence: Dict, 
        models: List[ModelSpecialization]
    ) -> Dict[str, Any]:
        """Cross-validate findings across models"""
        # This would implement cross-model evaluation
        return {
            "evaluations": {
                "hypothesis_strength": {"hyp1": 0.8, "hyp2": 0.6, "hyp3": 0.7},
                "evidence_quality": 0.75,
                "consistency_score": 0.85
            },
            "disagreements": [],
            "consensus_areas": ["Area 1", "Area 2"]
        }
    
    async def _consensus_synthesis(
        self, 
        problem: str, 
        reasoning_chain: List, 
        models: List[ModelSpecialization], 
        require_consensus: bool
    ) -> Dict[str, Any]:
        """Synthesize final answer with consensus building"""
        # This would implement consensus synthesis
        return {
            "consensus_answer": "Final synthesized answer based on collaborative reasoning",
            "consensus_achieved": True,
            "confidence": 0.9,
            "alternative_solutions": ["Alternative 1", "Alternative 2"],
            "dissenting_opinions": []
        }
    
    async def _final_validation(
        self, 
        problem: str, 
        synthesis: Dict, 
        models: List[ModelSpecialization]
    ) -> Dict[str, Any]:
        """Final validation of the synthesized answer"""
        # This would implement final validation
        return {
            "validation_passed": True,
            "validation_confidence": 0.85,
            "potential_issues": [],
            "recommendations": ["Recommendation 1"]
        }
    
    async def _metacognitive_analysis(
        self, 
        problem: str, 
        reasoning_chain: List, 
        models: List[ModelSpecialization]
    ) -> Dict[str, Any]:
        """Metacognitive analysis of the reasoning process"""
        # This would implement metacognitive analysis
        return {
            "reasoning_quality": 0.85,
            "identified_biases": ["Bias 1", "Bias 2"],
            "identified_limitations": ["Limitation 1"],
            "improvement_suggestions": ["Suggestion 1"],
            "confidence_calibration": 0.8
        }
    
    def _calculate_reasoning_confidence(self, reasoning_chain: List) -> float:
        """Calculate overall confidence in the reasoning process"""
        # Weight confidence scores from different stages
        stage_weights = {
            ReasoningStage.PROBLEM_ANALYSIS: 0.15,
            ReasoningStage.HYPOTHESIS_GENERATION: 0.15,
            ReasoningStage.EVIDENCE_GATHERING: 0.20,
            ReasoningStage.CRITICAL_EVALUATION: 0.20,
            ReasoningStage.SYNTHESIS: 0.20,
            ReasoningStage.VALIDATION: 0.10
        }
        
        weighted_confidence = 0.0  # Accumulator for weighted sum
        for stage_data in reasoning_chain:
            stage = stage_data.get("stage")
            if stage in stage_weights:
                stage_confidence = stage_data.get("results", {}).get("confidence", 0.5)
                weighted_confidence += stage_weights[stage] * stage_confidence
        
        return min(1.0, weighted_confidence)
    
    def _calculate_quality_metrics(self, reasoning_chain: List) -> Dict[str, float]:
        """Calculate quality metrics for the reasoning process"""
        return {
            "coherence": 0.85,
            "completeness": 0.80,
            "creativity": 0.75,
            "accuracy": 0.90,
            "efficiency": 0.70
        }
    
    async def _extract_common_themes(self, analyses: Dict) -> List[str]:
        """Extract common themes from individual analyses"""
        # This would use NLP to find common themes
        return ["Common theme 1", "Common theme 2"]
    
    async def _extract_divergent_perspectives(self, analyses: Dict) -> List[str]:
        """Extract divergent perspectives from analyses"""
        # This would identify key differences
        return ["Divergent view 1", "Divergent view 2"]
    
    async def get_status(self) -> Dict[str, Any]:
        """Get current status of the reasoning chain"""
        return {
            "agent_id": self.agent_id,
            "status": "operational",
            "capabilities": [
                "collaborative_reasoning",
                "chain_of_thought",
                "cross_validation",
                "metacognitive_analysis",
                "debate_reasoning",
                "consensus_building"
            ],
            "supported_models": [model.value for model in ModelSpecialization],
            "reasoning_types": [rt.value for rt in ReasoningType],
            "reasoning_stages": [rs.value for rs in ReasoningStage],
            "last_activity": self._get_timestamp()
        }
    
    def _get_timestamp(self) -> str:
        """Get current timestamp in ISO format"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    async def _conduct_debate_round(
        self,
        problem: str,
        positions: List[str], 
        round_num: int,
        debate_history: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Conduct a single round of structured debate between models
        
        Args:
            problem: The problem being debated
            positions: Current positions in the debate
            round_num: Current round number
            debate_history: Previous rounds of debate
            
        Returns:
            Results from this debate round
        """
        try:
            # Get specialized models for debate
            debate_models = [ModelSpecialization.CLAUDE_OPUS, ModelSpecialization.GPT4_TURBO]
            
            round_arguments = {}
            
            for i, position in enumerate(positions):
                model = debate_models[i % len(debate_models)]
                model_name = model.value
                
                # Build context from previous rounds
                context = ""
                if debate_history:
                    context = f"\nPrevious debate rounds:\n{json.dumps(debate_history, indent=2)}"
                
                debate_prompt = f"""
                You are participating in a structured debate about: {problem}
                
                Your position: {position}
                Round: {round_num + 1}
                {context}
                
                Provide your strongest argument for this position. Address counter-arguments from previous rounds if applicable.
                Be logical, evidence-based, and persuasive. Keep your response focused and under 300 words.
                """
                
                try:
                    response = await self.llm_provider.generate_response([
                        {"role": "system", "content": "You are an expert debater skilled in logical argumentation."},
                        {"role": "user", "content": debate_prompt}
                    ], agent_type="reasoning", requested_provider=model_name)
                    
                    round_arguments[f"position_{i}_{model_name}"] = {
                        "position": position,
                        "argument": response.get("content", "No argument provided"),
                        "model": model_name,
                        "confidence": response.get("confidence", 0.7)
                    }
                    
                except Exception as e:
                    logger.warning(f"Debate argument generation failed for {model_name}: {e}")
                    round_arguments[f"position_{i}_{model_name}"] = {
                        "position": position,
                        "argument": f"Unable to generate argument: {str(e)}",
                        "model": model_name,
                        "confidence": 0.1
                    }
            
            return {
                "round": round_num,
                "arguments": round_arguments,
                "timestamp": self._get_timestamp()
            }
            
        except Exception as e:
            logger.error(f"Debate round conduct failed: {e}")
            return {
                "round": round_num,
                "arguments": {},
                "error": str(e),
                "timestamp": self._get_timestamp()
            }
    
    async def _generate_debate_positions(self, problem: str) -> List[str]:
        """
        Generate initial debate positions for a problem
        
        Args:
            problem: The problem to generate positions for
            
        Returns:
            List of debate positions
        """
        try:
            position_prompt = f"""
            For the following problem/question, generate 2-3 distinct, reasonable positions that could be debated:
            
            Problem: {problem}
            
            Provide diverse perspectives that reasonable people might hold. Each position should be:
            - Clear and specific
            - Defensible with evidence
            - Meaningfully different from the others
            
            Format as a JSON list of position strings.
            """
            
            response = await self.llm_provider.generate_response([
                {"role": "system", "content": "You are an expert at identifying debate positions on complex topics."},
                {"role": "user", "content": position_prompt}
            ])
            
            content = response.get("content", "")
            
            try:
                positions = json.loads(content)
                if isinstance(positions, list) and len(positions) >= 2:
                    return positions[:3]  # Max 3 positions
            except json.JSONDecodeError:
                pass
            
            # Fallback to simple pro/con positions
            return [
                f"Position supporting: {problem}",
                f"Position opposing: {problem}"
            ]
            
        except Exception as e:
            logger.warning(f"Position generation failed: {e}")
            return [
                f"Affirmative position on: {problem}",
                f"Negative position on: {problem}"
            ]
    
    async def _update_positions(
        self, 
        original_positions: List[str], 
        round_results: Dict[str, Any]
    ) -> List[str]:
        """
        Update debate positions based on round results
        
        Args:
            original_positions: Original positions
            round_results: Results from the latest round
            
        Returns:
            Updated positions (may be unchanged)
        """
        # For now, keep positions stable across rounds
        # In future versions, could evolve positions based on strong counter-arguments
        return original_positions
    
    async def _synthesize_debate(
        self, 
        problem: str, 
        debate_history: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Synthesize final conclusions from debate history
        
        Args:
            problem: The original problem
            debate_history: Complete debate history
            
        Returns:
            Synthesis results with consensus analysis
        """
        try:
            # Build synthesis prompt with all debate content
            history_text = ""
            for round_data in debate_history:
                history_text += f"\nRound {round_data['round'] + 1}:\n"
                for arg_key, arg_data in round_data.get('arguments', {}).items():
                    history_text += f"- {arg_data['position']}: {arg_data['argument']}\n"
            
            synthesis_prompt = f"""
            Analyze this debate and provide a balanced synthesis:
            
            Problem: {problem}
            
            Debate History:{history_text}
            
            Provide:
            1. Whether consensus was reached (true/false)
            2. The strongest arguments presented
            3. Areas of agreement and disagreement
            4. A balanced conclusion
            
            Format as JSON with keys: consensus, strongest_arguments, agreements, disagreements, conclusion
            """
            
            response = await self.llm_provider.generate_response([
                {"role": "system", "content": "You are an expert at synthesizing debate outcomes objectively."},
                {"role": "user", "content": synthesis_prompt}
            ])
            
            content = response.get("content", "")
            
            try:
                synthesis = json.loads(content)
                return synthesis
            except json.JSONDecodeError:
                pass
            
            # Fallback synthesis
            return {
                "consensus": False,
                "strongest_arguments": ["Multiple valid perspectives presented"],
                "agreements": ["Complex issue with merit on multiple sides"],
                "disagreements": ["Fundamental differences in approach"],
                "conclusion": "The debate revealed important considerations from multiple perspectives."
            }
            
        except Exception as e:
            logger.error(f"Debate synthesis failed: {e}")
            return {
                "consensus": False,
                "strongest_arguments": [],
                "agreements": [],
                "disagreements": [],
                "conclusion": f"Synthesis failed: {str(e)}",
                "error": str(e)
            }

    def _generate_offline_reasoning(
        self,
        problem: str,
        reasoning_type: Optional[ReasoningType]
    ) -> Dict[str, Any]:
        """Generate deterministic reasoning output without LLM access"""
        reasoning_chain = []
        timestamp = self._get_timestamp()

        stages = [
            (ReasoningStage.PROBLEM_ANALYSIS, "Break problem into core components"),
            (ReasoningStage.HYPOTHESIS_GENERATION, "Enumerate possible approaches"),
            (ReasoningStage.EVIDENCE_GATHERING, "List required evidence or data"),
            (ReasoningStage.CRITICAL_EVALUATION, "Compare approaches and evaluate trade-offs"),
            (ReasoningStage.SYNTHESIS, "Recommend best approach with justification"),
            (ReasoningStage.VALIDATION, "Describe how to validate the recommendation"),
            (ReasoningStage.METACOGNITION, "Reflect on assumptions, risks, and next steps")
        ]

        for stage, prompt in stages:
            reasoning_chain.append({
                "stage": stage,
                "results": {
                    "analysis": f"{prompt} for: {problem}",
                    "confidence": 0.5,
                    "notes": "Generated via offline heuristic reasoning"
                },
                "timestamp": timestamp
            })

        return {
            "status": "offline",
            "problem": problem,
            "reasoning_type": reasoning_type.value if reasoning_type else "analytical",
            "reasoning_chain": reasoning_chain,
            "final_answer": f"Offline recommendation for: {problem}",
            "confidence": 0.5,
            "quality_metrics": {
                "coherence": 0.6,
                "completeness": 0.55,
                "creativity": 0.4,
                "accuracy": 0.5,
                "efficiency": 0.7
            },
            "models_used": ["offline_heuristic"],
            "reasoning_time": 0.1,
            "consensus_achieved": True,
            "alternative_solutions": [],
            "limitations": ["LLM providers unavailable; used deterministic heuristics"],
            "timestamp": timestamp
        }

    def _generate_offline_debate(
        self,
        problem: str,
        positions: Optional[List[str]],
        rounds: int
    ) -> Dict[str, Any]:
        if not positions:
            positions = [
                f"Support for {problem}",
                f"Opposition to {problem}"
            ]

        debate_history = []
        timestamp = self._get_timestamp()

        for round_num in range(rounds):
            arguments = {}
            for idx, position in enumerate(positions):
                arguments[f"position_{idx}"] = {
                    "position": position,
                    "argument": f"Offline argument for position '{position}' in round {round_num + 1}",
                    "confidence": 0.45,
                    "model": "offline_heuristic"
                }
            debate_history.append({
                "round": round_num,
                "arguments": arguments,
                "timestamp": timestamp
            })

        return {
            "status": "offline",
            "problem": problem,
            "debate_rounds": len(debate_history),
            "debate_history": debate_history,
            "final_synthesis": {
                "consensus": True,
                "summary": "Offline debate concluded using heuristic reasoning",
                "strongest_arguments": [arguments for round_data in debate_history for arguments in round_data["arguments"].values()]
            },
            "consensus_reached": True,
            "timestamp": timestamp
        }