"""
Multi-LLM Agent - Week 4 Implementation

This module implements an advanced multi-LLM agent that orchestrates multiple
language models for enhanced reasoning, validation, and response generation.
Integrates with the LLM Provider Manager for physics-informed context routing.

Enhanced Features (v3):
- Complete self-audit integration with real-time integrity monitoring
- Mathematical validation of multi-LLM coordination operations with evidence-based metrics
- Comprehensive integrity oversight for all multi-LLM outputs
- Auto-correction capabilities for coordination-related communications

Key Features:
- Multi-provider orchestration with intelligent routing
- Physics-informed context enhancement
- Response validation and consensus building
- Specialized task delegation to optimal providers
- Real-time performance monitoring and adaptation
- Cost optimization with quality assurance
"""

import asyncio
import logging
import time
from typing import Dict, Any, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
from collections import defaultdict

from ...core.agent import NISAgent, NISLayer
from ..hybrid_agent_core import CompleteScientificProcessingResult, CompleteHybridAgent
from ...llm.providers.llm_provider_manager import (
    LLMProviderManager, PhysicsInformedContext, LLMResponse, FusedResponse,
    TaskType, LLMProvider, ResponseConfidence
)

# Integrity metrics for actual calculations
from src.utils.integrity_metrics import (
    calculate_confidence, create_default_confidence_factors, ConfidenceFactors
)

# Self-audit capabilities for real-time integrity monitoring
from src.utils.self_audit import self_audit_engine, ViolationType, IntegrityViolation

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MultiLLMStrategy(Enum):
    """Strategies for multi-LLM coordination."""
    CONSENSUS = "consensus"           # Seek agreement among providers
    SPECIALIST = "specialist"        # Route to best specialist for task
    ENSEMBLE = "ensemble"           # Combine all providers equally
    VALIDATION = "validation"       # Use multiple providers for validation
    CREATIVE_FUSION = "creative_fusion"  # Blend creative and analytical approaches
    PHYSICS_INFORMED = "physics_informed"  # Physics-compliance-driven routing

class ValidationLevel(Enum):
    """Levels of multi-LLM validation."""
    BASIC = "basic"           # Single provider
    STANDARD = "standard"     # 2-3 providers with consensus
    RIGOROUS = "rigorous"     # 3-4 providers with detailed validation
    MAXIMUM = "maximum"       # All available providers with full analysis

@dataclass
class MultiLLMTask:
    """Task definition for multi-LLM processing."""
    task_id: str
    prompt: str
    task_type: TaskType
    strategy: MultiLLMStrategy
    validation_level: ValidationLevel
    max_providers: int = 3
    physics_requirements: Dict[str, float] = field(default_factory=dict)
    priority: str = "normal"
    timeout: float = 60.0
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class MultiLLMResult:
    """Result from multi-LLM processing."""
    task_id: str
    primary_response: str
    confidence: float
    providers_used: List[LLMProvider]
    consensus_score: float
    physics_compliance: float
    validation_results: Dict[str, Any]
    processing_time: float
    total_cost: float
    strategy_used: MultiLLMStrategy
    individual_responses: List[LLMResponse] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)

class MultiLLMAgent(NISAgent):
    """
    Multi-LLM Agent for orchestrated language model coordination.
    
    This agent manages multiple LLM providers to achieve enhanced reasoning,
    validation, and response generation through intelligent coordination
    and physics-informed context routing.
    """
    
    def __init__(self, agent_id: str = "multi_llm_001",
                 default_strategy: MultiLLMStrategy = MultiLLMStrategy.PHYSICS_INFORMED,
                 enable_self_audit: bool = True):
        super().__init__(agent_id, NISLayer.REASONING, "Multi-LLM coordination agent")
        
        # Core components
        self.provider_manager = LLMProviderManager()
        self.default_strategy = default_strategy
        
        # Configuration
        self.max_concurrent_requests = 5
        self.default_timeout = 60.0
        self.physics_threshold = 0.8
        self.consensus_threshold = 0.7
        
        # Performance tracking
        self.coordination_stats = {
            "total_tasks": 0,
            "successful_tasks": 0,
            "average_processing_time": 0.0,
            "average_consensus": 0.0,
            "average_physics_compliance": 0.0,
            "strategy_usage": {strategy.value: 0 for strategy in MultiLLMStrategy},
            "provider_effectiveness": {},
            "cost_efficiency": 0.0
        }
        
        # Task queue and processing
        self.task_queue = asyncio.Queue()
        self.active_tasks = {}
        self.task_history = []
        
        # Set up self-audit integration
        self.enable_self_audit = enable_self_audit
        self.integrity_monitoring_enabled = enable_self_audit
        self.integrity_metrics = {
            'monitoring_start_time': time.time(),
            'total_outputs_monitored': 0,
            'total_violations_detected': 0,
            'auto_corrections_applied': 0,
            'average_integrity_score': 100.0
        }
        
        # Initialize confidence factors for mathematical validation
        self.confidence_factors = create_default_confidence_factors()
        
        self.logger = logging.getLogger(f"nis.multi_llm.{agent_id}")
        self.logger.info(f"Initialized Multi-LLM Agent: {agent_id} with self-audit: {enable_self_audit}")
    
    def process(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process multi-LLM coordination requests.
        
        Args:
            message: Input message with operation and payload
            
        Returns:
            Processed response with multi-LLM coordination results
        """
        try:
            operation = message.get("operation", "coordinate")
            payload = message.get("payload", {})
            
            if operation == "coordinate":
                return self._coordinate_multi_llm(payload)
            elif operation == "create_task":
                return self._create_multi_llm_task(payload)
            elif operation == "validate_response":
                return self._validate_with_multi_llm(payload)
            elif operation == "get_statistics":
                return self._get_coordination_statistics(payload)
            elif operation == "optimize_strategy":
                return self._optimize_coordination_strategy(payload)
            else:
                return self._create_error_response(f"Unknown operation: {operation}")
                
        except Exception as e:
            self.logger.error(f"Error in multi-LLM coordination: {str(e)}")
            return self._create_error_response(str(e))
    
    def _coordinate_multi_llm(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main multi-LLM coordination function.
        
        Args:
            payload: Contains task definition and scientific context
            
        Returns:
            Multi-LLM coordination results
        """
        try:
            # Extract task information
            prompt = payload.get("prompt", "")
            task_type = TaskType(payload.get("task_type", TaskType.SCIENTIFIC_ANALYSIS.value))
            strategy = MultiLLMStrategy(payload.get("strategy", self.default_strategy.value))
            
            # Extract scientific context
            scientific_result = payload.get("scientific_result", {})
            physics_compliance = scientific_result.get("physics_compliance", 1.0)
            physics_violations = scientific_result.get("physics_violations", [])
            symbolic_functions = scientific_result.get("symbolic_functions", [])
            scientific_insights = scientific_result.get("scientific_insights", [])
            integrity_score = scientific_result.get("integrity_score", 1.0)
            
            # Create task
            task = MultiLLMTask(
                task_id=f"task_{int(time.time()*1000)}",
                prompt=prompt,
                task_type=task_type,
                strategy=strategy,
                validation_level=ValidationLevel(payload.get("validation_level", ValidationLevel.STANDARD.value)),
                max_providers=payload.get("max_providers", 3),
                physics_requirements=payload.get("physics_requirements", {"min_compliance": self.physics_threshold}),
                priority=payload.get("priority", "normal"),
                metadata=payload.get("metadata", {})
            )
            
            # Create physics-informed context
            context = PhysicsInformedContext(
                original_prompt=prompt,
                physics_compliance=physics_compliance,
                physics_violations=physics_violations,
                symbolic_functions=symbolic_functions,
                scientific_insights=scientific_insights,
                integrity_score=integrity_score,
                task_type=task_type,
                priority=task.priority,
                metadata=task.metadata
            )
            
            # Execute coordination strategy
            result = asyncio.run(self._execute_coordination_strategy(task, context))
            
            # Update statistics
            self._update_coordination_stats(result)
            
            # Format response
            return self._create_response("success", {
                "task_id": result.task_id,
                "response": result.primary_response,
                "confidence": result.confidence,
                "consensus_score": result.consensus_score,
                "physics_compliance": result.physics_compliance,
                "providers_used": [p.value for p in result.providers_used],
                "strategy_used": result.strategy_used.value,
                "validation_results": result.validation_results,
                "processing_time": result.processing_time,
                "total_cost": result.total_cost,
                "recommendations": result.recommendations,
                "multi_llm_enhanced": True
            })
            
        except Exception as e:
            self.logger.error(f"Multi-LLM coordination failed: {e}")
            return self._create_error_response(f"Coordination failed: {str(e)}")
    
    async def _execute_coordination_strategy(self, task: MultiLLMTask, 
                                           context: PhysicsInformedContext) -> MultiLLMResult:
        """Execute the specified coordination strategy."""
        start_time = time.time()
        
        try:
            if task.strategy == MultiLLMStrategy.CONSENSUS:
                result = await self._consensus_strategy(task, context)
            elif task.strategy == MultiLLMStrategy.SPECIALIST:
                result = await self._specialist_strategy(task, context)
            elif task.strategy == MultiLLMStrategy.ENSEMBLE:
                result = await self._ensemble_strategy(task, context)
            elif task.strategy == MultiLLMStrategy.VALIDATION:
                result = await self._validation_strategy(task, context)
            elif task.strategy == MultiLLMStrategy.CREATIVE_FUSION:
                result = await self._creative_fusion_strategy(task, context)
            elif task.strategy == MultiLLMStrategy.PHYSICS_INFORMED:
                result = await self._physics_informed_strategy(task, context)
            else:
                # Default to physics-informed strategy
                result = await self._physics_informed_strategy(task, context)
            
            result.processing_time = time.time() - start_time
            result.strategy_used = task.strategy
            
            return result
            
        except Exception as e:
            self.logger.error(f"Strategy execution failed: {e}")
            # Return error result
            return MultiLLMResult(
                task_id=task.task_id,
                primary_response=f"Strategy execution failed: {str(e)}",
                confidence=0.0,
                providers_used=[],
                consensus_score=0.0,
                physics_compliance=0.0,
                validation_results={"error": str(e)},
                processing_time=time.time() - start_time,
                total_cost=0.0,
                strategy_used=task.strategy
            )
    
    async def _consensus_strategy(self, task: MultiLLMTask, 
                                context: PhysicsInformedContext) -> MultiLLMResult:
        """Consensus-based strategy seeking agreement among providers."""
        # Use multiple providers and seek consensus
        fused_response = await self.provider_manager.generate_response(
            context, use_fusion=True, max_providers=task.max_providers
        )
        
        # Enhanced consensus analysis
        consensus_details = self._analyze_consensus(fused_response)
        
        return MultiLLMResult(
            task_id=task.task_id,
            primary_response=fused_response.primary_response,
            confidence=fused_response.confidence,
            providers_used=fused_response.contributing_providers,
            consensus_score=fused_response.consensus_score,
            physics_compliance=context.physics_compliance,
            validation_results=consensus_details,
            processing_time=fused_response.processing_time,
            total_cost=fused_response.total_cost,
            strategy_used=task.strategy,
            individual_responses=fused_response.response_variations,
            recommendations=self._generate_consensus_recommendations(fused_response)
        )
    
    async def _specialist_strategy(self, task: MultiLLMTask, 
                                 context: PhysicsInformedContext) -> MultiLLMResult:
        """Specialist strategy routing to the best provider for the task."""
        # Use single best provider
        response = await self.provider_manager.generate_response(
            context, use_fusion=False
        )
        
        return MultiLLMResult(
            task_id=task.task_id,
            primary_response=response.response_text,
            confidence=response.confidence,
            providers_used=[response.provider],
            consensus_score=1.0,  # Single provider = perfect consensus
            physics_compliance=context.physics_compliance,
            validation_results={"specialist_selected": response.provider.value},
            processing_time=response.processing_time,
            total_cost=response.cost,
            strategy_used=task.strategy,
            individual_responses=[response],
            recommendations=[f"Optimal specialist: {response.provider.value}"]
        )
    
    async def _ensemble_strategy(self, task: MultiLLMTask, 
                               context: PhysicsInformedContext) -> MultiLLMResult:
        """Ensemble strategy combining all available providers equally."""
        # Use all available providers
        fused_response = await self.provider_manager.generate_response(
            context, use_fusion=True, max_providers=len(self.provider_manager.providers)
        )
        
        # Enhanced ensemble analysis
        ensemble_details = self._analyze_ensemble(fused_response)
        
        return MultiLLMResult(
            task_id=task.task_id,
            primary_response=fused_response.primary_response,
            confidence=fused_response.confidence,
            providers_used=fused_response.contributing_providers,
            consensus_score=fused_response.consensus_score,
            physics_compliance=context.physics_compliance,
            validation_results=ensemble_details,
            processing_time=fused_response.processing_time,
            total_cost=fused_response.total_cost,
            strategy_used=task.strategy,
            individual_responses=fused_response.response_variations,
            recommendations=self._generate_ensemble_recommendations(fused_response)
        )
    
    async def _validation_strategy(self, task: MultiLLMTask, 
                                 context: PhysicsInformedContext) -> MultiLLMResult:
        """Validation strategy using multiple providers for cross-validation."""
        # Primary response
        primary_response = await self.provider_manager.generate_response(
            context, use_fusion=False
        )
        
        # Validation responses
        validation_context = PhysicsInformedContext(
            original_prompt=f"Validate this response: {primary_response.response_text}",
            physics_compliance=context.physics_compliance,
            physics_violations=context.physics_violations,
            symbolic_functions=context.symbolic_functions,
            scientific_insights=context.scientific_insights,
            integrity_score=context.integrity_score,
            task_type=TaskType.PHYSICS_VALIDATION,
            priority=context.priority,
            metadata=context.metadata
        )
        
        validation_response = await self.provider_manager.generate_response(
            validation_context, use_fusion=True, max_providers=task.max_providers - 1
        )
        
        # Combine primary and validation
        combined_confidence = (primary_response.confidence + validation_response.confidence) / 2
        total_cost = primary_response.cost + validation_response.total_cost
        
        validation_details = {
            "primary_provider": primary_response.provider.value,
            "validation_providers": [p.value for p in validation_response.contributing_providers],
            "validation_consensus": validation_response.consensus_score,
            "cross_validation_score": self._calculate_cross_validation_score(primary_response, validation_response)
        }
        
        return MultiLLMResult(
            task_id=task.task_id,
            primary_response=primary_response.response_text,
            confidence=combined_confidence,
            providers_used=[primary_response.provider] + validation_response.contributing_providers,
            consensus_score=validation_response.consensus_score,
            physics_compliance=context.physics_compliance,
            validation_results=validation_details,
            processing_time=primary_response.processing_time + validation_response.processing_time,
            total_cost=total_cost,
            strategy_used=task.strategy,
            individual_responses=[primary_response] + validation_response.response_variations,
            recommendations=self._generate_validation_recommendations(validation_details)
        )
    
    async def _creative_fusion_strategy(self, task: MultiLLMTask, 
                                      context: PhysicsInformedContext) -> MultiLLMResult:
        """Creative fusion strategy blending creative and analytical approaches."""
        # Modify context for creative exploration
        creative_context = PhysicsInformedContext(
            original_prompt=f"Explore creative approaches to: {context.original_prompt}",
            physics_compliance=context.physics_compliance,
            physics_violations=context.physics_violations,
            symbolic_functions=context.symbolic_functions,
            scientific_insights=context.scientific_insights,
            integrity_score=context.integrity_score,
            task_type=TaskType.CREATIVE_EXPLORATION,
            priority=context.priority,
            metadata=context.metadata
        )
        
        # Get creative responses
        creative_response = await self.provider_manager.generate_response(
            creative_context, use_fusion=True, max_providers=2
        )
        
        # Get analytical validation
        analytical_context = PhysicsInformedContext(
            original_prompt=f"Analyze and validate: {creative_response.primary_response}",
            physics_compliance=context.physics_compliance,
            physics_violations=context.physics_violations,
            symbolic_functions=context.symbolic_functions,
            scientific_insights=context.scientific_insights,
            integrity_score=context.integrity_score,
            task_type=TaskType.SCIENTIFIC_ANALYSIS,
            priority=context.priority,
            metadata=context.metadata
        )
        
        analytical_response = await self.provider_manager.generate_response(
            analytical_context, use_fusion=False
        )
        
        # Fuse creative and analytical
        fused_response = self._fuse_creative_analytical(creative_response, analytical_response)
        
        return MultiLLMResult(
            task_id=task.task_id,
            primary_response=fused_response["response"],
            confidence=fused_response["confidence"],
            providers_used=creative_response.contributing_providers + [analytical_response.provider],
            consensus_score=fused_response["consensus"],
            physics_compliance=context.physics_compliance,
            validation_results=fused_response["validation_details"],
            processing_time=creative_response.processing_time + analytical_response.processing_time,
            total_cost=creative_response.total_cost + analytical_response.cost,
            strategy_used=task.strategy,
            individual_responses=creative_response.response_variations + [analytical_response],
            recommendations=fused_response["recommendations"]
        )
    
    async def _physics_informed_strategy(self, task: MultiLLMTask, 
                                       context: PhysicsInformedContext) -> MultiLLMResult:
        """Physics-informed strategy optimizing for physics compliance."""
        # Route based on physics requirements
        if context.physics_compliance < self.physics_threshold:
            # Use validation-focused providers
            context.recommended_provider = LLMProvider.CLAUDE4
            context.task_type = TaskType.PHYSICS_VALIDATION
        elif context.task_type == TaskType.CREATIVE_EXPLORATION:
            # Use creative providers but with physics awareness
            context.recommended_provider = LLMProvider.GEMINI_PRO
        else:
            # Use high-capability providers
            context.recommended_provider = LLMProvider.GPT4_1
        
        # Generate response with physics optimization
        response = await self.provider_manager.generate_response(
            context, use_fusion=True, max_providers=task.max_providers
        )
        
        # Physics-specific validation
        physics_validation = self._validate_physics_compliance(response, context)
        
        return MultiLLMResult(
            task_id=task.task_id,
            primary_response=response.primary_response,
            confidence=response.confidence,
            providers_used=response.contributing_providers,
            consensus_score=response.consensus_score,
            physics_compliance=context.physics_compliance,
            validation_results=physics_validation,
            processing_time=response.processing_time,
            total_cost=response.total_cost,
            strategy_used=task.strategy,
            individual_responses=response.response_variations,
            recommendations=self._generate_physics_recommendations(physics_validation, context)
        )
    
    def _analyze_consensus(self, fused_response: FusedResponse) -> Dict[str, Any]:
        """Analyze consensus details from fused response."""
        return {
            "consensus_method": fused_response.fusion_method,
            "provider_agreement": fused_response.consensus_score,
            "confidence_variance": np.var([r.confidence for r in fused_response.response_variations]),
            "response_similarity": self._calculate_response_similarity(fused_response.response_variations),
            "physics_consistency": fused_response.physics_validated
        }
    
    def _analyze_ensemble(self, fused_response: FusedResponse) -> Dict[str, Any]:
        """Analyze ensemble details from fused response."""
        return {
            "ensemble_size": len(fused_response.contributing_providers),
            "provider_diversity": len(set(fused_response.contributing_providers)),
            "confidence_distribution": [r.confidence for r in fused_response.response_variations],
            "cost_efficiency": fused_response.total_cost / len(fused_response.response_variations),
            "processing_efficiency": fused_response.processing_time / len(fused_response.response_variations)
        }
    
    def _calculate_cross_validation_score(self, primary: LLMResponse, 
                                        validation: FusedResponse) -> float:
        """Calculate cross-validation score between primary and validation responses."""
        # Simple similarity-based validation score
        confidence_alignment = 1.0 - abs(primary.confidence - validation.confidence)
        consensus_factor = validation.consensus_score
        
        return (confidence_alignment + consensus_factor) / 2
    
    def _fuse_creative_analytical(self, creative: FusedResponse, 
                                analytical: LLMResponse) -> Dict[str, Any]:
        """Fuse creative and analytical responses."""
        # Combine responses with creative-analytical balance
        fused_text = f"ðŸŽ¨ Creative Exploration:\n{creative.primary_response}\n\nðŸ”¬ Analytical Validation:\n{analytical.response_text}"
        
        # Calculate balanced confidence
        creative_weight = 0.6 if creative.consensus_score > 0.7 else 0.4
        analytical_weight = 1.0 - creative_weight
        
        fused_confidence = (
            creative.confidence * creative_weight +
            analytical.confidence * analytical_weight
        )
        
        consensus = (creative.consensus_score + 1.0) / 2  # Analytical has "consensus" of 1.0
        
        return {
            "response": fused_text,
            "confidence": fused_confidence,
            "consensus": consensus,
            "validation_details": {
                "creative_consensus": creative.consensus_score,
                "analytical_confidence": analytical.confidence,
                "fusion_balance": f"{creative_weight:.1f} creative / {analytical_weight:.1f} analytical"
            },
            "recommendations": [
                "Creative-analytical fusion applied",
                f"Creative consensus: {creative.consensus_score:.2f}",
                f"Analytical validation: {analytical.confidence:.2f}"
            ]
        }
    
    def _validate_physics_compliance(self, response: FusedResponse, 
                                   context: PhysicsInformedContext) -> Dict[str, Any]:
        """Validate physics compliance of the response."""
        return {
            "physics_score": context.physics_compliance,
            "violations_count": len(context.physics_violations),
            "physics_enhanced": response.physics_validated,
            "compliance_threshold": self.physics_threshold,
            "passes_physics_check": context.physics_compliance >= self.physics_threshold,
            "symbolic_functions_count": len(context.symbolic_functions),
            "integrity_score": context.integrity_score
        }
    
    def _calculate_response_similarity(self, responses: List[LLMResponse]) -> float:
        """Calculate similarity among responses (simplified implementation)."""
        if len(responses) < 2:
            return 1.0
        
        # Simple length-based similarity (could be enhanced with semantic analysis)
        lengths = [len(r.response_text) for r in responses]
        length_variance = np.var(lengths) / (np.mean(lengths) + 1)
        
        # Convert variance to similarity (lower variance = higher similarity)
        similarity = max(0.0, 1.0 - length_variance)
        
        return similarity
    
    def _generate_consensus_recommendations(self, fused_response: FusedResponse) -> List[str]:
        """Generate recommendations for consensus strategy."""
        recommendations = []
        
        if fused_response.consensus_score > 0.8:
            recommendations.append("High consensus achieved - high confidence in result")
        elif fused_response.consensus_score > 0.6:
            recommendations.append("Moderate consensus - consider additional validation")
        else:
            recommendations.append("Low consensus - significant disagreement among providers")
        
        if fused_response.physics_validated:
            recommendations.append("Physics validation confirmed across providers")
        
        return recommendations
    
    def _generate_ensemble_recommendations(self, fused_response: FusedResponse) -> List[str]:
        """Generate recommendations for ensemble strategy."""
        recommendations = []
        
        provider_count = len(fused_response.contributing_providers)
        recommendations.append(f"Ensemble of {provider_count} providers used")
        
        if fused_response.total_cost > 0.05:  # Arbitrary threshold
            recommendations.append("High cost - consider optimizing provider selection")
        
        return recommendations
    
    def _generate_validation_recommendations(self, validation_details: Dict[str, Any]) -> List[str]:
        """Generate recommendations for validation strategy."""
        recommendations = []
        
        cross_val_score = validation_details["cross_validation_score"]
        if cross_val_score > 0.8:
            recommendations.append("Strong cross-validation - high reliability")
        else:
            recommendations.append("Weak cross-validation - consider additional review")
        
        return recommendations
    
    def _generate_physics_recommendations(self, physics_validation: Dict[str, Any], 
                                        context: PhysicsInformedContext) -> List[str]:
        """Generate recommendations for physics-informed strategy."""
        recommendations = []
        
        if physics_validation["passes_physics_check"]:
            recommendations.append("Physics compliance validated")
        else:
            recommendations.append("Physics compliance below threshold - review required")
        
        if context.physics_violations:
            recommendations.append(f"{len(context.physics_violations)} physics violations detected")
        
        return recommendations
    
    def _update_coordination_stats(self, result: MultiLLMResult):
        """Update coordination statistics."""
        self.coordination_stats["total_tasks"] += 1
        
        if result.confidence > 0.6:
            self.coordination_stats["successful_tasks"] += 1
        
        # Update averages
        total = self.coordination_stats["total_tasks"]
        
        self.coordination_stats["average_processing_time"] = (
            (self.coordination_stats["average_processing_time"] * (total - 1) + result.processing_time) / total
        )
        
        self.coordination_stats["average_consensus"] = (
            (self.coordination_stats["average_consensus"] * (total - 1) + result.consensus_score) / total
        )
        
        self.coordination_stats["average_physics_compliance"] = (
            (self.coordination_stats["average_physics_compliance"] * (total - 1) + result.physics_compliance) / total
        )
        
        # Update strategy usage
        self.coordination_stats["strategy_usage"][result.strategy_used.value] += 1
        
        # Update cost efficiency
        if result.total_cost > 0:
            efficiency = result.confidence / result.total_cost
            current_efficiency = self.coordination_stats["cost_efficiency"]
            self.coordination_stats["cost_efficiency"] = (
                (current_efficiency * (total - 1) + efficiency) / total
            )
    
    def _create_multi_llm_task(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Create a multi-LLM task."""
        # Implementation for task creation
        return self._create_response("success", {"task_created": True})
    
    def _validate_with_multi_llm(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Validate a response using multiple LLMs."""
        # Implementation for multi-LLM validation
        return self._create_response("success", {"validation_complete": True})
    
    def _get_coordination_statistics(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Get coordination statistics."""
        total_tasks = self.coordination_stats["total_tasks"]
        success_rate = 0.0
        if total_tasks > 0:
            success_rate = self.coordination_stats["successful_tasks"] / total_tasks
        
        return self._create_response("success", {
            **self.coordination_stats,
            "success_rate": success_rate,
            "agent_id": self.agent_id,
            "provider_manager_stats": self.provider_manager.get_global_statistics()
        })
    
    def _optimize_coordination_strategy(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Optimize coordination strategy based on performance data."""
        # Implementation for strategy optimization
        return self._create_response("success", {"optimization_complete": True})

# Example usage and testing
if __name__ == "__main__":
    import asyncio
    
    async def test_multi_llm_agent():
        """Test the Multi-LLM Agent."""
        agent = MultiLLMAgent()
        
        # Test multi-LLM coordination
        test_message = {
            "operation": "coordinate",
            "payload": {
                "prompt": "Analyze the stability and energy conservation of a damped harmonic oscillator",
                "task_type": "scientific_analysis",
                "strategy": "physics_informed",
                "validation_level": "standard",
                "max_providers": 3,
                "scientific_result": {
                    "physics_compliance": 0.85,
                    "physics_violations": ["minor energy dissipation anomaly"],
                    "symbolic_functions": ["sin(2*pi*t)*exp(-0.1*t)"],
                    "scientific_insights": ["Damped oscillation", "Energy conservation with dissipation"],
                    "integrity_score": 0.88
                }
            }
        }
        
        result = agent.process(test_message)
        
        if result["status"] == "success":
            payload = result["payload"]
            print(f"ðŸ¤– Multi-LLM Coordination Results:")
            print(f"   Confidence: {payload['confidence']:.3f}")
            print(f"   Consensus Score: {payload['consensus_score']:.3f}")
            print(f"   Physics Compliance: {payload['physics_compliance']:.3f}")
            print(f"   Providers Used: {payload['providers_used']}")
            print(f"   Strategy: {payload['strategy_used']}")
            print(f"   Cost: ${payload['total_cost']:.4f}")
            print(f"   Processing Time: {payload['processing_time']:.3f}s")
        else:
            print(f"âŒ Multi-LLM coordination failed: {result['payload']}")
    
    asyncio.run(test_multi_llm_agent()) 
    
    # ==================== COMPREHENSIVE SELF-AUDIT CAPABILITIES ====================
    
    def audit_multi_llm_output(self, output_text: str, operation: str = "", context: str = "") -> Dict[str, Any]:
        """
        Perform real-time integrity audit on multi-LLM coordination outputs.
        
        Args:
            output_text: Text output to audit
            operation: Multi-LLM operation type (coordinate, validate, optimize, etc.)
            context: Additional context for the audit
            
        Returns:
            Audit results with violations and integrity score
        """
        if not self.enable_self_audit:
            return {'integrity_score': 100.0, 'violations': [], 'total_violations': 0}
        
        self.logger.info(f"Performing self-audit on multi-LLM output for operation: {operation}")
        
        # Use proven audit engine
        audit_context = f"multi_llm:{operation}:{context}" if context else f"multi_llm:{operation}"
        violations = self_audit_engine.audit_text(output_text, audit_context)
        integrity_score = self_audit_engine.get_integrity_score(output_text)
        
        # Log violations for multi-LLM-specific analysis
        if violations:
            self.logger.warning(f"Detected {len(violations)} integrity violations in multi-LLM output")
            for violation in violations:
                self.logger.warning(f"  - {violation.severity}: {violation.text} -> {violation.suggested_replacement}")
        
        return {
            'violations': violations,
            'integrity_score': integrity_score,
            'total_violations': len(violations),
            'violation_breakdown': self._categorize_multi_llm_violations(violations),
            'operation': operation,
            'audit_timestamp': time.time()
        }
    
    def auto_correct_multi_llm_output(self, output_text: str, operation: str = "") -> Dict[str, Any]:
        """
        Automatically correct integrity violations in multi-LLM outputs.
        
        Args:
            output_text: Text to correct
            operation: Multi-LLM operation type
            
        Returns:
            Corrected output with audit details
        """
        if not self.enable_self_audit:
            return {'corrected_text': output_text, 'violations_fixed': [], 'improvement': 0}
        
        self.logger.info(f"Performing self-correction on multi-LLM output for operation: {operation}")
        
        corrected_text, violations = self_audit_engine.auto_correct_text(output_text)
        
        # Calculate improvement metrics with mathematical validation
        original_score = self_audit_engine.get_integrity_score(output_text)
        corrected_score = self_audit_engine.get_integrity_score(corrected_text)
        improvement = calculate_confidence(corrected_score - original_score, self.confidence_factors)
        
        # Update integrity metrics
        if hasattr(self, 'integrity_metrics'):
            self.integrity_metrics['auto_corrections_applied'] += len(violations)
        
        return {
            'original_text': output_text,
            'corrected_text': corrected_text,
            'violations_fixed': violations,
            'original_integrity_score': original_score,
            'corrected_integrity_score': corrected_score,
            'improvement': improvement,
            'operation': operation,
            'correction_timestamp': time.time()
        }
    
    def analyze_multi_llm_integrity_trends(self, time_window: int = 3600) -> Dict[str, Any]:
        """
        Analyze multi-LLM coordination integrity trends for self-improvement.
        
        Args:
            time_window: Time window in seconds to analyze
            
        Returns:
            Multi-LLM integrity trend analysis with mathematical validation
        """
        if not self.enable_self_audit:
            return {'integrity_status': 'MONITORING_DISABLED'}
        
        self.logger.info(f"Analyzing multi-LLM integrity trends over {time_window} seconds")
        
        # Get integrity report from audit engine
        integrity_report = self_audit_engine.generate_integrity_report()
        
        # Calculate multi-LLM-specific metrics
        multi_llm_metrics = {
            'default_strategy': self.default_strategy.value,
            'max_concurrent_requests': self.max_concurrent_requests,
            'physics_threshold': self.physics_threshold,
            'consensus_threshold': self.consensus_threshold,
            'provider_manager_configured': bool(self.provider_manager),
            'coordination_stats': self.coordination_stats,
            'task_queue_size': self.task_queue.qsize(),
            'active_tasks_count': len(self.active_tasks),
            'task_history_length': len(self.task_history)
        }
        
        # Generate multi-LLM-specific recommendations
        recommendations = self._generate_multi_llm_integrity_recommendations(
            integrity_report, multi_llm_metrics
        )
        
        return {
            'integrity_status': integrity_report['integrity_status'],
            'total_violations': integrity_report['total_violations'],
            'multi_llm_metrics': multi_llm_metrics,
            'integrity_trend': self._calculate_multi_llm_integrity_trend(),
            'recommendations': recommendations,
            'analysis_timestamp': time.time()
        }
    
    def get_multi_llm_integrity_report(self) -> Dict[str, Any]:
        """Generate comprehensive multi-LLM coordination integrity report"""
        if not self.enable_self_audit:
            return {'status': 'SELF_AUDIT_DISABLED'}
        
        # Get basic integrity report
        base_report = self_audit_engine.generate_integrity_report()
        
        # Add multi-LLM-specific metrics
        multi_llm_report = {
            'multi_llm_agent_id': self.agent_id,
            'monitoring_enabled': self.integrity_monitoring_enabled,
            'multi_llm_capabilities': {
                'multi_provider_orchestration': True,
                'physics_informed_routing': True,
                'consensus_building': True,
                'specialized_task_delegation': True,
                'performance_monitoring': True,
                'cost_optimization': True,
                'supported_strategies': [strategy.value for strategy in MultiLLMStrategy],
                'default_strategy': self.default_strategy.value
            },
            'configuration_status': {
                'provider_manager_initialized': bool(self.provider_manager),
                'max_concurrent_requests': self.max_concurrent_requests,
                'physics_threshold': self.physics_threshold,
                'consensus_threshold': self.consensus_threshold,
                'default_timeout': self.default_timeout
            },
            'processing_statistics': {
                'total_tasks': self.coordination_stats.get('total_tasks', 0),
                'successful_tasks': self.coordination_stats.get('successful_tasks', 0),
                'average_processing_time': self.coordination_stats.get('average_processing_time', 0.0),
                'average_consensus': self.coordination_stats.get('average_consensus', 0.0),
                'average_physics_compliance': self.coordination_stats.get('average_physics_compliance', 0.0),
                'cost_efficiency': self.coordination_stats.get('cost_efficiency', 0.0),
                'strategy_usage': self.coordination_stats.get('strategy_usage', {}),
                'task_queue_size': self.task_queue.qsize(),
                'active_tasks': len(self.active_tasks),
                'task_history_entries': len(self.task_history)
            },
            'integrity_metrics': getattr(self, 'integrity_metrics', {}),
            'base_integrity_report': base_report,
            'report_timestamp': time.time()
        }
        
        return multi_llm_report
    
    def validate_multi_llm_configuration(self) -> Dict[str, Any]:
        """Validate multi-LLM coordination configuration for integrity"""
        validation_results = {
            'valid': True,
            'warnings': [],
            'recommendations': []
        }
        
        # Check provider manager
        if not self.provider_manager:
            validation_results['valid'] = False
            validation_results['warnings'].append("Provider manager not initialized")
            validation_results['recommendations'].append("Initialize LLMProviderManager for multi-LLM coordination")
        
        # Check thresholds
        if self.physics_threshold <= 0 or self.physics_threshold >= 1:
            validation_results['warnings'].append("Invalid physics threshold - should be between 0 and 1")
            validation_results['recommendations'].append("Set physics_threshold to a value between 0.5-0.9")
        
        if self.consensus_threshold <= 0 or self.consensus_threshold >= 1:
            validation_results['warnings'].append("Invalid consensus threshold - should be between 0 and 1")
            validation_results['recommendations'].append("Set consensus_threshold to a value between 0.5-0.9")
        
        # Check concurrent requests
        if self.max_concurrent_requests <= 0:
            validation_results['warnings'].append("Invalid max concurrent requests - must be positive")
            validation_results['recommendations'].append("Set max_concurrent_requests to a positive value (e.g., 5)")
        
        # Check success rate
        success_rate = (self.coordination_stats.get('successful_tasks', 0) / 
                       max(1, self.coordination_stats.get('total_tasks', 1)))
        
        if success_rate < 0.8:
            validation_results['warnings'].append(f"Low success rate: {success_rate:.1%}")
            validation_results['recommendations'].append("Investigate and optimize coordination strategies for better success rate")
        
        # Check cost efficiency
        cost_efficiency = self.coordination_stats.get('cost_efficiency', 0.0)
        if cost_efficiency < 0.5:
            validation_results['warnings'].append(f"Low cost efficiency: {cost_efficiency:.2f}")
            validation_results['recommendations'].append("Optimize provider selection and strategy usage for better cost efficiency")
        
        return validation_results
    
    def _monitor_multi_llm_output_integrity(self, output_text: str, operation: str = "") -> str:
        """
        Internal method to monitor and potentially correct multi-LLM output integrity.
        
        Args:
            output_text: Output to monitor
            operation: Multi-LLM operation type
            
        Returns:
            Potentially corrected output
        """
        if not getattr(self, 'integrity_monitoring_enabled', False):
            return output_text
        
        # Perform audit
        audit_result = self.audit_multi_llm_output(output_text, operation)
        
        # Update monitoring metrics
        if hasattr(self, 'integrity_metrics'):
            self.integrity_metrics['total_outputs_monitored'] += 1
            self.integrity_metrics['total_violations_detected'] += audit_result['total_violations']
        
        # Auto-correct if violations detected
        if audit_result['violations']:
            correction_result = self.auto_correct_multi_llm_output(output_text, operation)
            
            self.logger.info(f"Auto-corrected multi-LLM output: {len(audit_result['violations'])} violations fixed")
            
            return correction_result['corrected_text']
        
        return output_text
    
    def _categorize_multi_llm_violations(self, violations: List[IntegrityViolation]) -> Dict[str, int]:
        """Categorize integrity violations specific to multi-LLM operations"""
        categories = defaultdict(int)
        
        for violation in violations:
            categories[violation.violation_type.value] += 1
        
        return dict(categories)
    
    def _generate_multi_llm_integrity_recommendations(self, integrity_report: Dict[str, Any], multi_llm_metrics: Dict[str, Any]) -> List[str]:
        """Generate multi-LLM-specific integrity improvement recommendations"""
        recommendations = []
        
        if integrity_report.get('total_violations', 0) > 5:
            recommendations.append("Consider implementing more rigorous multi-LLM output validation")
        
        if not multi_llm_metrics.get('provider_manager_configured', False):
            recommendations.append("Initialize LLM Provider Manager for multi-LLM coordination")
        
        if multi_llm_metrics.get('task_queue_size', 0) > 100:
            recommendations.append("Task queue is large - consider increasing concurrent processing capacity")
        
        if multi_llm_metrics.get('active_tasks_count', 0) > 20:
            recommendations.append("Many active tasks - monitor resource usage and performance")
        
        success_rate = (multi_llm_metrics.get('coordination_stats', {}).get('successful_tasks', 0) / 
                       max(1, multi_llm_metrics.get('coordination_stats', {}).get('total_tasks', 1)))
        
        if success_rate < 0.8:
            recommendations.append("Low coordination success rate - consider optimizing strategies and provider selection")
        
        avg_consensus = multi_llm_metrics.get('coordination_stats', {}).get('average_consensus', 0.0)
        if avg_consensus < 0.7:
            recommendations.append("Low average consensus - consider adjusting consensus threshold or provider selection")
        
        cost_efficiency = multi_llm_metrics.get('coordination_stats', {}).get('cost_efficiency', 0.0)
        if cost_efficiency < 0.5:
            recommendations.append("Low cost efficiency - optimize provider usage and task allocation")
        
        if multi_llm_metrics.get('physics_threshold', 0) < 0.7:
            recommendations.append("Physics threshold is low - consider increasing for better physics compliance")
        
        if len(recommendations) == 0:
            recommendations.append("Multi-LLM coordination integrity status is excellent - maintain current practices")
        
        return recommendations
    
    def _calculate_multi_llm_integrity_trend(self) -> Dict[str, Any]:
        """Calculate multi-LLM integrity trends with mathematical validation"""
        if not hasattr(self, 'coordination_stats'):
            return {'trend': 'INSUFFICIENT_DATA'}
        
        total_tasks = self.coordination_stats.get('total_tasks', 0)
        successful_tasks = self.coordination_stats.get('successful_tasks', 0)
        
        if total_tasks == 0:
            return {'trend': 'NO_TASKS_PROCESSED'}
        
        success_rate = successful_tasks / total_tasks
        avg_consensus = self.coordination_stats.get('average_consensus', 0.0)
        avg_physics_compliance = self.coordination_stats.get('average_physics_compliance', 0.0)
        cost_efficiency = self.coordination_stats.get('cost_efficiency', 0.0)
        avg_processing_time = self.coordination_stats.get('average_processing_time', 0.0)
        
        # Calculate trend with mathematical validation
        processing_efficiency = 1.0 / max(avg_processing_time, 0.1)
        trend_score = calculate_confidence(
            (success_rate * 0.3 + avg_consensus * 0.2 + avg_physics_compliance * 0.2 + 
             cost_efficiency * 0.2 + min(processing_efficiency / 10.0, 1.0) * 0.1), 
            self.confidence_factors
        )
        
        return {
            'trend': 'IMPROVING' if trend_score > 0.8 else 'STABLE' if trend_score > 0.6 else 'NEEDS_ATTENTION',
            'success_rate': success_rate,
            'avg_consensus': avg_consensus,
            'avg_physics_compliance': avg_physics_compliance,
            'cost_efficiency': cost_efficiency,
            'avg_processing_time': avg_processing_time,
            'trend_score': trend_score,
            'tasks_processed': total_tasks,
            'coordination_analysis': self._analyze_multi_llm_patterns()
        }
    
    def _analyze_multi_llm_patterns(self) -> Dict[str, Any]:
        """Analyze multi-LLM coordination patterns for integrity assessment"""
        if not hasattr(self, 'coordination_stats') or not self.coordination_stats:
            return {'pattern_status': 'NO_COORDINATION_STATS'}
        
        total_tasks = self.coordination_stats.get('total_tasks', 0)
        successful_tasks = self.coordination_stats.get('successful_tasks', 0)
        strategy_usage = self.coordination_stats.get('strategy_usage', {})
        
        # Find most used strategy
        most_used_strategy = max(strategy_usage.items(), key=lambda x: x[1])[0] if strategy_usage else "none"
        
        return {
            'pattern_status': 'NORMAL' if total_tasks > 0 else 'NO_COORDINATION_ACTIVITY',
            'total_tasks': total_tasks,
            'successful_tasks': successful_tasks,
            'success_rate': successful_tasks / max(1, total_tasks),
            'most_used_strategy': most_used_strategy,
            'strategy_distribution': strategy_usage,
            'task_queue_current_size': self.task_queue.qsize(),
            'active_tasks_current': len(self.active_tasks),
            'analysis_timestamp': time.time()
        }

# Bind the methods to the MultiLLMAgent class
MultiLLMAgent.audit_multi_llm_output = audit_multi_llm_output
MultiLLMAgent.auto_correct_multi_llm_output = auto_correct_multi_llm_output
MultiLLMAgent.analyze_multi_llm_integrity_trends = analyze_multi_llm_integrity_trends
MultiLLMAgent.get_multi_llm_integrity_report = get_multi_llm_integrity_report
MultiLLMAgent.validate_multi_llm_configuration = validate_multi_llm_configuration
MultiLLMAgent._monitor_multi_llm_output_integrity = _monitor_multi_llm_output_integrity
MultiLLMAgent._categorize_multi_llm_violations = _categorize_multi_llm_violations
MultiLLMAgent._generate_multi_llm_integrity_recommendations = _generate_multi_llm_integrity_recommendations
MultiLLMAgent._calculate_multi_llm_integrity_trend = _calculate_multi_llm_integrity_trend
MultiLLMAgent._analyze_multi_llm_patterns = _analyze_multi_llm_patterns

if __name__ == "__main__":
    import asyncio
    
    async def test_multi_llm_agent():
        """Test the Multi-LLM Agent."""
        agent = MultiLLMAgent()
        
        # Test multi-LLM coordination
        test_message = {
            "operation": "coordinate",
            "payload": {
                "prompt": "Analyze the stability and energy conservation of a damped harmonic oscillator",
                "task_type": "scientific_analysis",
                "strategy": "physics_informed",
                "validation_level": "standard",
                "max_providers": 3,
                "scientific_result": {
                    "physics_compliance": 0.85,
                    "physics_violations": ["minor energy dissipation anomaly"],
                    "symbolic_functions": ["sin(2*pi*t)*exp(-0.1*t)"],
                    "scientific_insights": ["Damped oscillation", "Energy conservation with dissipation"],
                    "integrity_score": 0.88
                }
            }
        }
        
        result = agent.process(test_message)
        
        if result["status"] == "success":
            payload = result["payload"]
            print(f"ðŸ¤– Multi-LLM Coordination Results:")
            print(f"   Confidence: {payload['confidence']:.3f}")
            print(f"   Consensus Score: {payload['consensus_score']:.3f}")
            print(f"   Physics Compliance: {payload['physics_compliance']:.3f}")
            print(f"   Providers Used: {payload['providers_used']}")
            print(f"   Strategy: {payload['strategy_used']}")
            print(f"   Cost: ${payload['total_cost']:.4f}")
            print(f"   Processing Time: {payload['processing_time']:.3f}s")
        else:
            print(f"âŒ Multi-LLM coordination failed: {result['payload']}")
    
    asyncio.run(test_multi_llm_agent()) 