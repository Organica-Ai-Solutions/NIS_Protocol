# ðŸ“Š NIS Protocol Analytics Complete Guide

> **AWS CloudWatch Style Analytics for LLM Usage**
> 
> Comprehensive monitoring, cost optimization, and performance analytics for your NIS Protocol deployment.

## ðŸŽ¯ **Overview**

The NIS Protocol Analytics system provides enterprise-grade monitoring and optimization for LLM usage, similar to AWS CloudWatch but specifically designed for AI workloads. Built on Redis for real-time performance and scalability.

### **Key Benefits:**
- **60-75% cost reduction** through intelligent caching and optimization
- **Real-time monitoring** of token usage, costs, and performance
- **AWS-style dashboards** with comprehensive metrics and charts
- **Smart provider routing** based on cost, speed, and quality requirements
- **Automated optimization** with user-controllable consensus modes

---

## ðŸš€ **Quick Start**

### **1. Setup Redis**
```bash
# Install Redis
brew install redis  # macOS
sudo apt-get install redis-server  # Ubuntu

# Start Redis
redis-server

# Install Python Redis client
pip install redis
```

### **2. Start NIS Protocol with Analytics**
```bash
cd /path/to/NIS_Protocol
python main.py
```

### **3. Test Analytics Endpoints**
```bash
# Quick overview
curl "http://localhost:8000/analytics"

# Detailed cost analysis
curl "http://localhost:8000/analytics?view=costs&include_charts=true"

# Real-time monitoring
curl "http://localhost:8000/analytics?view=realtime"
```

---

## ðŸ“Š **Analytics Architecture**

### **Data Flow:**
```
User Request â†’ LLM Manager â†’ Analytics Engine â†’ Redis â†’ Dashboard
                â†“              â†“               â†“
            Smart Cache â†’ Rate Limiter â†’ Cost Tracker
```

### **Storage Strategy:**
- **Memory Cache (TTL)**: Ultra-fast access for recent data
- **Redis Cache**: Persistent storage with automatic expiration
- **Analytics DB**: Separate Redis database for historical data
- **Aggregated Stats**: Hourly, daily, and provider-specific summaries

### **Real-time Processing:**
- **Token counting**: Input/output token tracking per request
- **Cost calculation**: Real-time cost estimation by provider
- **Performance metrics**: Latency, cache hits, error rates
- **Usage patterns**: Trending and forecasting analysis

---

## ðŸŽ® **Using the Analytics System**

### **Primary Endpoint: `/analytics`**

The unified analytics endpoint provides 8 different views of your LLM usage data:

#### **1. Summary View (Default)**
```bash
curl "http://localhost:8000/analytics"
curl "http://localhost:8000/analytics?view=summary&hours_back=24"
```

**Returns:**
- High-level overview metrics
- Top provider by usage
- Most cost-efficient provider
- System health alerts
- Cache hit rate summary

#### **2. Detailed View**
```bash
curl "http://localhost:8000/analytics?view=detailed&hours_back=48"
```

**Returns:**
- Complete usage metrics
- Provider breakdown
- Token analysis
- User analytics
- Recent activity logs
- System health scores

#### **3. Token Analytics**
```bash
curl "http://localhost:8000/analytics?view=tokens&hours_back=24"
```

**Returns:**
- Input vs output token ratios
- Provider-specific token usage
- Agent type breakdown
- Efficiency ratings
- Optimization recommendations

#### **4. Cost Analytics**
```bash
curl "http://localhost:8000/analytics?view=costs&include_charts=true"
```

**Returns:**
- Financial summary and projections
- Provider cost comparison
- Savings from optimization
- Budget analysis
- Cost trends over time

#### **5. Performance Analytics**
```bash
curl "http://localhost:8000/analytics?view=performance"
```

**Returns:**
- Response time metrics
- Cache efficiency
- Throughput analysis
- Provider performance rankings
- Performance grades

#### **6. Real-time Monitoring**
```bash
curl "http://localhost:8000/analytics?view=realtime"
```

**Returns:**
- Live current hour metrics
- Recent request details
- System status indicators
- Active provider count

#### **7. Provider Comparison**
```bash
curl "http://localhost:8000/analytics?view=providers"
```

**Returns:**
- Side-by-side provider comparison
- Rankings by speed, cost, usage
- Provider recommendations
- Capability analysis

#### **8. Trends & Forecasting**
```bash
curl "http://localhost:8000/analytics?view=trends&hours_back=168"
```

**Returns:**
- Historical patterns
- Trend analysis (increasing/stable/decreasing)
- Peak usage identification
- Next-hour forecasting
- Daily cost projections

### **Advanced Filtering**

#### **Filter by Provider**
```bash
# OpenAI-specific analytics
curl "http://localhost:8000/analytics?view=costs&provider=openai"

# Anthropic performance analysis
curl "http://localhost:8000/analytics?view=performance&provider=anthropic"
```

#### **Custom Time Ranges**
```bash
# Last hour
curl "http://localhost:8000/analytics?view=summary&hours_back=1"

# Last week
curl "http://localhost:8000/analytics?view=trends&hours_back=168"

# Last 48 hours with charts
curl "http://localhost:8000/analytics?view=costs&hours_back=48&include_charts=true"
```

#### **Chart Data for Visualization**
```bash
# Include chart data for dashboards
curl "http://localhost:8000/analytics?view=detailed&include_charts=true"
```

**Chart data includes:**
- Hourly request patterns
- Cost trends over time
- Provider usage distribution
- Latency performance trends

---

## ðŸ’° **Cost Optimization Features**

### **Smart Caching**
- **Memory + Redis**: Two-tier caching for optimal performance
- **Provider-specific TTL**: Expensive providers cached longer
- **Semantic similarity**: Find similar cached responses
- **Cost-aware policies**: Cache valuable responses preferentially

### **Rate Limiting**
- **Provider-specific limits**: Respect API quotas and avoid overage
- **Priority queuing**: Critical requests processed first
- **Sliding window**: Smooth request distribution
- **Emergency throttling**: Automatic slowdown during high usage

### **User-Controllable Consensus**
- **Single mode**: Fastest, cheapest (1 provider)
- **Dual mode**: Validation with 2 providers
- **Triple mode**: Strong consensus with 3 providers
- **Smart mode**: AI chooses optimal strategy
- **Custom mode**: User selects specific providers

### **Intelligent Provider Routing**
- **Cost-based routing**: Cheaper providers for simple tasks
- **Quality-based routing**: Premium providers for complex tasks
- **Speed-based routing**: Fastest providers for time-sensitive requests
- **Balanced routing**: Optimal cost/quality/speed tradeoffs

---

## ðŸ“ˆ **Key Metrics Explained**

### **Token Metrics**
- **Input Tokens**: Tokens in your prompts (cost factor)
- **Output Tokens**: Tokens in AI responses (higher cost factor)
- **Total Tokens**: Sum of input + output tokens
- **Token Efficiency**: Input/output ratio (lower is better)
- **Tokens per Request**: Average tokens consumed per API call

### **Cost Metrics**
- **Current Cost**: Actual spending in time period
- **Projected Monthly**: Estimated monthly cost based on current usage
- **Cache Savings**: Money saved through caching
- **Cost per Request**: Average cost per API call
- **Cost per Token**: Average cost per token by provider

### **Performance Metrics**
- **Average Latency**: Response time from request to completion
- **Cache Hit Rate**: Percentage of requests served from cache
- **Error Rate**: Percentage of failed requests
- **Throughput**: Requests processed per hour
- **System Health**: Overall system performance score

### **Usage Patterns**
- **Peak Hours**: Times of highest usage
- **Request Trends**: Usage increasing/stable/decreasing
- **Provider Distribution**: Which providers are used most
- **Agent Activity**: Which agents generate most requests

---

## ðŸŽ¯ **Optimization Strategies**

### **Cost Reduction (60-75% savings possible)**

#### **Enable Intelligent Caching**
```bash
# High-value caching
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "Your query", "enable_caching": true, "consensus_mode": "smart"}'
```

#### **Use Appropriate Consensus Modes**
```bash
# Simple queries: single provider
curl -X POST "http://localhost:8000/chat" \
  -d '{"message": "What is 2+2?", "consensus_mode": "single", "provider": "google"}'

# Important decisions: dual consensus
curl -X POST "http://localhost:8000/chat" \
  -d '{"message": "Strategic analysis", "consensus_mode": "dual"}'

# Critical analysis: smart mode (AI chooses)
curl -X POST "http://localhost:8000/chat" \
  -d '{"message": "Complex research", "consensus_mode": "smart"}'
```

#### **Provider Selection Strategy**
- **Google/DeepSeek**: Cost-effective for simple queries
- **OpenAI**: Balanced performance for general use
- **Anthropic**: High-quality reasoning and analysis
- **NVIDIA**: Specialized physics and simulation tasks

### **Performance Optimization**

#### **Cache Tuning**
- Monitor cache hit rates in analytics
- Adjust TTL settings for your usage patterns
- Use semantic similarity for related queries
- Clear cache for specific providers when needed

#### **Rate Limiting Optimization**
- Set appropriate priority levels for requests
- Monitor queue times in analytics
- Adjust limits based on provider performance
- Use emergency throttling for cost control

### **Monitoring Best Practices**

#### **Daily Monitoring**
```bash
# Daily cost check
curl "http://localhost:8000/analytics?view=costs&hours_back=24"

# Performance review
curl "http://localhost:8000/analytics?view=performance"

# System health check
curl "http://localhost:8000/analytics?view=realtime"
```

#### **Weekly Analysis**
```bash
# Weekly trends
curl "http://localhost:8000/analytics?view=trends&hours_back=168"

# Provider comparison
curl "http://localhost:8000/analytics?view=providers"

# Detailed analysis
curl "http://localhost:8000/analytics?view=detailed&hours_back=168&include_charts=true"
```

---

## ðŸ”§ **Configuration & Customization**

### **Redis Configuration**
```python
# Custom Redis settings
from src.analytics.llm_analytics import init_llm_analytics
from src.llm.smart_cache import init_smart_cache

# Analytics on Redis DB 1
analytics = init_llm_analytics(
    redis_host="localhost",
    redis_port=6379,
    redis_db=1
)

# Cache on Redis DB 0
cache = init_smart_cache(
    redis_host="localhost",
    redis_port=6379,
    redis_db=0,
    max_size=2000,
    default_ttl=7200
)
```

### **Provider Cost Configuration**
```python
# Custom provider costs (per 1K tokens)
provider_costs = {
    "openai": {"input": 0.0025, "output": 0.0075},
    "anthropic": {"input": 0.003, "output": 0.015},
    "google": {"input": 0.000375, "output": 0.0015},
    "deepseek": {"input": 0.00055, "output": 0.002},
    "nvidia": {"input": 0.005, "output": 0.015}
}
```

### **Cache TTL Settings**
```python
# Provider-specific cache settings
provider_settings = {
    "openai": {"ttl": 3600, "cost_weight": 1.0},      # 1 hour
    "anthropic": {"ttl": 7200, "cost_weight": 1.5},   # 2 hours (expensive)
    "google": {"ttl": 1800, "cost_weight": 0.3},      # 30 min (cheap)
    "nvidia": {"ttl": 7200, "cost_weight": 2.0}       # 2 hours (premium)
}
```

---

## ðŸš¨ **Troubleshooting**

### **Common Issues**

#### **Redis Connection Errors**
```bash
# Check Redis is running
redis-cli ping

# Should return: PONG
```

#### **No Analytics Data**
```bash
# Check if requests are being made
curl -X POST "http://localhost:8000/chat" \
  -d '{"message": "test analytics"}'

# Then check analytics
curl "http://localhost:8000/analytics?view=realtime"
```

#### **Cache Not Working**
```bash
# Clear cache and test
curl -X POST "http://localhost:8000/llm/cache/clear"

# Test with caching enabled
curl -X POST "http://localhost:8000/chat" \
  -d '{"message": "test cache", "enable_caching": true}'
```

### **Analytics Cleanup**
```bash
# Clean old data (keep last 30 days)
curl -X POST "http://localhost:8000/analytics/cleanup?days_to_keep=30"
```

### **Redis Memory Management**
```bash
# Check Redis memory usage
redis-cli info memory

# Configure Redis max memory
redis-cli config set maxmemory 2gb
redis-cli config set maxmemory-policy allkeys-lru
```

---

## ðŸ“Š **Analytics Dashboard Integration**

### **Building Custom Dashboards**

The analytics endpoints provide JSON data perfect for dashboards like Grafana, custom React apps, or any visualization tool.

#### **Sample Dashboard Data**
```bash
# Get chart-ready data
curl "http://localhost:8000/analytics?view=detailed&include_charts=true"
```

**Returns chart data:**
```json
{
  "charts": {
    "hourly_requests": [
      {"hour": "2025-01-19:14", "requests": 45},
      {"hour": "2025-01-19:15", "requests": 52}
    ],
    "hourly_costs": [
      {"hour": "2025-01-19:14", "cost": 0.032},
      {"hour": "2025-01-19:15", "cost": 0.041}
    ],
    "provider_distribution": [
      {"provider": "openai", "requests": 120},
      {"provider": "anthropic", "requests": 85}
    ],
    "latency_trends": [
      {"hour": "2025-01-19:14", "latency": 1250},
      {"hour": "2025-01-19:15", "latency": 1180}
    ]
  }
}
```

#### **Grafana Integration Example**
```bash
# Grafana HTTP data source configuration
URL: http://localhost:8000/analytics
Method: GET
Parameters: view=detailed&include_charts=true&hours_back=24
```

---

## ðŸŽ¯ **Best Practices**

### **Cost Management**
1. **Enable caching** for all non-critical requests
2. **Use single provider mode** for simple queries
3. **Monitor daily costs** and set alerts
4. **Choose providers wisely** based on task complexity
5. **Regular cleanup** of old analytics data

### **Performance Optimization**
1. **Monitor cache hit rates** (target >60%)
2. **Track response times** by provider
3. **Use rate limiting** to prevent API overages
4. **Monitor error rates** and investigate spikes
5. **Adjust TTL settings** based on usage patterns

### **Monitoring Strategy**
1. **Daily checks**: Cost, performance, errors
2. **Weekly analysis**: Trends, optimization opportunities
3. **Monthly reviews**: Provider performance, cost optimization
4. **Real-time alerts**: Error spikes, cost overruns
5. **Capacity planning**: Usage trends and scaling needs

---

## ðŸ“š **API Reference Summary**

### **Main Analytics Endpoint**
```
GET /analytics?view={view}&hours_back={hours}&provider={provider}&include_charts={bool}
```

**Views:** `summary`, `detailed`, `tokens`, `costs`, `performance`, `realtime`, `providers`, `trends`

### **Legacy Endpoints**
```
GET /analytics/dashboard      # AWS-style main dashboard
GET /analytics/tokens        # Token usage analysis
GET /analytics/costs         # Cost breakdown
GET /analytics/performance   # Performance metrics
GET /analytics/realtime      # Live monitoring
POST /analytics/cleanup      # Data cleanup
```

### **Cache Management**
```
POST /llm/cache/clear                    # Clear cache
GET /llm/optimization/stats              # Optimization statistics
POST /llm/consensus/configure            # Configure consensus
GET /llm/providers/recommendations       # Provider recommendations
```

---

**ðŸ“Š Analytics Status:** âœ… Fully Operational
**ðŸ”§ Redis Required:** Yes (localhost:6379 default)
**ðŸ’° Cost Savings:** 60-75% typical reduction
**ðŸ“ˆ Real-time:** Yes, sub-second updates
**ðŸŽ¯ Coverage:** Complete LLM usage tracking

*For detailed API documentation, see [Complete API Reference](../api/COMPLETE_API_REFERENCE.md)*
